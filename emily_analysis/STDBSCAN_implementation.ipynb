{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "import pyproj\n",
    "import seaborn as sns\n",
    "import geopandas as gpd \n",
    "import shapely.geometry as geometry\n",
    "import shapely.ops as ops \n",
    "from functools import partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to initiate the STDBSCAN class \n",
    "\n",
    "class STDBSCAN(object):\n",
    "\n",
    "    def __init__(self, col_lat, col_lon, col_time, spatial_threshold=500.0, \n",
    "                 temporal_threshold=60.0, min_neighbors=15):\n",
    "        \"\"\"\n",
    "        Python st-dbscan implementation.\n",
    "        :param col_lat: Latitude column name;\n",
    "        :param col_lon:  Longitude column name;\n",
    "        :param col_time: Date time column name;\n",
    "        :param spatial_threshold: Maximum geographical coordinate (spatial)\n",
    "             distance value (meters);\n",
    "        :param temporal_threshold: Maximum non-spatial distance value (seconds);\n",
    "        :param min_neighbors: Minimum number of points within Eps1 and Eps2\n",
    "             distance;\n",
    "        \"\"\"\n",
    "        self.col_lat = col_lat\n",
    "        self.col_lon = col_lon\n",
    "        self.col_time = col_time\n",
    "        self.spatial_threshold = spatial_threshold\n",
    "        self.temporal_threshold = temporal_threshold\n",
    "        self.min_neighbors = min_neighbors\n",
    "\n",
    "    def projection(self, df, p1_str='epsg:4326', p2_str='epsg:3395'):\n",
    "        \"\"\"\n",
    "        Cython wrapper to converts from geographic (longitude,latitude)\n",
    "        to native map projection (x,y) coordinates. It needs to select the\n",
    "        right epsg. Values of x and y are given in meters\n",
    "        \"\"\"\n",
    "        p1 = pyproj.Proj(init=p1_str)\n",
    "        p2 = pyproj.Proj(init=p2_str)\n",
    "        lon = df[self.col_lon].values\n",
    "        lat = df[self.col_lat].values\n",
    "        x1, y1 = p1(lon, lat)\n",
    "        x2, y2 = pyproj.transform(p1, p2, x1, y1, radians=True)\n",
    "        df[self.col_lon] = x2\n",
    "        df[self.col_lat] = y2\n",
    "\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def _retrieve_neighbors(self, index_center, matrix):\n",
    "\n",
    "        center_point = matrix[index_center, :]\n",
    "\n",
    "        # filter by time\n",
    "        min_time = center_point[2] - timedelta(seconds=self.temporal_threshold)\n",
    "        max_time = center_point[2] + timedelta(seconds=self.temporal_threshold)\n",
    "        matrix = matrix[(matrix[:, 2] >= min_time) &\n",
    "                        (matrix[:, 2] <= max_time), :]\n",
    "        # filter by distance\n",
    "        tmp = (matrix[:, 0]-center_point[0])*(matrix[:, 0]-center_point[0]) + \\\n",
    "            (matrix[:, 1]-center_point[1])*(matrix[:, 1]-center_point[1])\n",
    "        neigborhood = matrix[tmp <= (\n",
    "            self.spatial_threshold*self.spatial_threshold), 4].tolist()\n",
    "        neigborhood.remove(index_center)\n",
    "\n",
    "        return neigborhood\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            df={o1,o2,...,on} Set of objects;\n",
    "        OUTPUT:\n",
    "            C = {c1,c2,...,ck} Set of clusters\n",
    "        \"\"\"\n",
    "        cluster_label = 0\n",
    "        noise = -1\n",
    "        unmarked = 777777\n",
    "        stack = []\n",
    "\n",
    "        # initial setup\n",
    "        df = df[[self.col_lon, self.col_lat, self.col_time]]\n",
    "        df = df.assign(cluster=unmarked)\n",
    "        df['index'] = range(df.shape[0])\n",
    "        matrix = df.values\n",
    "        df.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "        # for each point in database\n",
    "        for index in range(matrix.shape[0]):\n",
    "            if matrix[index, 3] == unmarked:\n",
    "                neighborhood = self._retrieve_neighbors(index, matrix)\n",
    "\n",
    "                if len(neighborhood) < self.min_neighbors:\n",
    "                    matrix[index, 3] = noise\n",
    "                else:  # found a core point\n",
    "                    cluster_label += 1\n",
    "                    # assign a label to core point\n",
    "                    matrix[index, 3] = cluster_label\n",
    "\n",
    "                    # assign core's label to its neighborhood\n",
    "                    for neig_index in neighborhood:\n",
    "                        matrix[neig_index, 3] = cluster_label\n",
    "                        stack.append(neig_index)  # append neighbors to stack\n",
    "\n",
    "                    # find new neighbors from core point neighborhood\n",
    "                    while len(stack) > 0:\n",
    "                        current_point_index = stack.pop()\n",
    "                        new_neighborhood = \\\n",
    "                            self._retrieve_neighbors(current_point_index,\n",
    "                                                     matrix)\n",
    "\n",
    "                        # current_point is a new core\n",
    "                        if len(new_neighborhood) >= self.min_neighbors:\n",
    "                            for neig_index in new_neighborhood:\n",
    "                                neig_cluster = matrix[neig_index, 3]\n",
    "                                if any([neig_cluster == noise,\n",
    "                                        neig_cluster == unmarked]):\n",
    "                                    matrix[neig_index, 3] = cluster_label\n",
    "                                    stack.append(neig_index)\n",
    "\n",
    "        df['cluster'] = matrix[:, 3]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def plot_clusters(df, output_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    labels = df['cluster'].values\n",
    "    X = df[['longitude', 'latitude']].values\n",
    "\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('ST-DSCAN: #n of clusters {}'.format(len(unique_labels)))\n",
    "    plt.show()\n",
    "    # plt.savefig(output_name)\n",
    "\n",
    "\n",
    "def test_time(df):\n",
    "    '''\n",
    "    transfrom the lon and lat to x and y\n",
    "    need to select the right epsg\n",
    "    I don't the true epsg of sample, but get the same result by using \n",
    "    epsg:4326 and epsg:32635\n",
    "    '''\n",
    "    st_dbscan = STDBSCAN(col_lat='location_latitude', col_lon='location_longitude',\n",
    "                         col_time='time', spatial_threshold=0.03,\n",
    "                         temporal_threshold=60, min_neighbors=1)\n",
    "    #df = st_dbscan.projection(df, p1_str='epsg:4326', p2_str='epsg:32630')\n",
    "    result_t180 = st_dbscan.run(df)\n",
    "    return result_t180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transition points of the outages \n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages['time'] = outages['outage_time'].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "outages['time'] = pd.to_datetime(outages['time'], infer_datetime_format=True)\n",
    "outages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.DataFrame(test_time(outages))\n",
    "    print(pd.value_counts(df['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = pd.DataFrame(test_time(outages))\n",
    "clustered['core_id'] = outages['core_id']\n",
    "\n",
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "clustered['time'] = clustered['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "month_clust = pd.DataFrame(clustered.groupby(['cluster'])['time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = clustered.groupby(['cluster'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = clustered.groupby(['cluster'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['ids'] = clustered.groupby(['cluster'])['core_id'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_clust['cluster_size'] = clustered.groupby(['cluster'])['time'].count()\n",
    "month_clust['min_time'] = month_clust['time'].apply(lambda x: min(x))\n",
    "month_clust['max_time'] = month_clust['time'].apply(lambda x: max(x))\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['time_range'], bins=30)\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Times in a Cluster')\n",
    "plt.xlabel('Time Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['lat_range'])\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Latitude in a Cluster')\n",
    "plt.xlabel('Latitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['long_range'])\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Longitude in a Cluster')\n",
    "plt.xlabel('Longitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all of the data from July 2018 (not just outage transition points!) \n",
    "#only use this for SAIFI (and convex hull caclulations)\n",
    "#I have confirmed that there are no duplicate points that read as powered and not powered \n",
    "pw = pd.read_parquet('part-00000-602cb425-c6be-40be-8024-aeb92fcb4315-c000.gz.parquet').drop(['product_id', 'millis', 'last_unplug_millis'], axis=1)\n",
    "pw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calculate SAIFI \n",
    "st_SAIFI_num = len(clustered[clustered['cluster'] != -1])\n",
    "st_SAIFI_denom = len(pw['core_id'].unique())*(len(month_clust))\n",
    "st_SAIFI = st_SAIFI_num/st_SAIFI_denom\n",
    "st_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if powered points are within the time range of the clusters \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage\n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = []\n",
    "within_time_long = []\n",
    "for o_index in range(len(month_clust)): \n",
    "    nest_lat = []\n",
    "    nest_long= []\n",
    "    for p_index in range(len(powered)): \n",
    "        if powered['time'].values[p_index] >= month_clust['min_time'].values[o_index] and powered['time'].values[p_index] <= month_clust['max_time'].values[o_index]:\n",
    "            nest_lat.append(powered['location_latitude'].values[p_index])\n",
    "            nest_long.append(powered['location_longitude'].values[p_index])        \n",
    "    within_time_lat.append(nest_lat)\n",
    "    within_time_long.append(nest_long)\n",
    "            \n",
    "month_clust['within_time_lat'] = within_time_lat\n",
    "month_clust['within_time_long'] = within_time_long\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = month_clust['latitude'].copy()\n",
    "update_long = month_clust['longitude'].copy()\n",
    "update_within_lat = month_clust['within_time_lat'].copy()\n",
    "update_within_long = month_clust['within_time_long'].copy()\n",
    "\n",
    "for i in range(len(month_clust)): \n",
    "    if len(month_clust.iloc[i]['within_time_lat']) == 0: \n",
    "        update_within_lat.values[i] = [0, 1, 2]\n",
    "        update_within_long.values[i] = [0, 1, 2] \n",
    "    if len(update_within_lat.values[i]) < 3: \n",
    "        update_within_lat.values[i] = month_clust.iloc[i]['within_time_lat']*3\n",
    "        update_within_long.values[i] = month_clust.iloc[i]['within_time_long']*3        \n",
    "    if month_clust.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = month_clust.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = month_clust.iloc[i]['longitude']*2\n",
    "\n",
    "month_clust['latitude'] = update_lat\n",
    "month_clust['longitude'] = update_long\n",
    "month_clust['within_time_long'] = update_within_long\n",
    "month_clust['within_time_lat'] = update_within_lat\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create geodataframes to calculate convex hull \n",
    "power = month_clust.copy()\n",
    "out = month_clust.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(month_clust)):\n",
    "    a = month_clust.iloc[i, :]['within_time_long']\n",
    "    b = month_clust.iloc[i, :]['within_time_lat']\n",
    "    c = month_clust.iloc[i, :]['longitude']\n",
    "    d = month_clust.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "month_clust['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "month_clust['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "month_clust['powered_poly'] = (np.vectorize(unique_coords)(month_clust['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "month_clust['outage_poly'] = (np.vectorize(unique_coords)(month_clust['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "out.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot size of outage vs. % out at within the convex hull \n",
    "def outage_size(outage_coords): \n",
    "    return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x='cluster_size', y='percent_pow_within_outage', data=out)\n",
    "plt.title('STDBSCAN: Number of Sensors in Outage vs. Percent of Sensors Experiencing Outage Within the Convex Hull of the Outage')\n",
    "plt.xlabel('Number of Sensors in an Outage')\n",
    "plt.ylabel('Percentage of Sensors Powered within Convex Hull')\n",
    "\n",
    "plt.ylim((-2.863820561337118, 60.00667770419427))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out['percent_pow_within_outage'], bins=20)\n",
    "plt.title('STDBSCAN: percentage of powered sensors within the convex hull of an outage')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_right = out[out['percent_pow_within_outage'] > 9]\n",
    "# la = []\n",
    "# lo = [] \n",
    "# for i in range(len(top_right['powered_within_outage'].values[0])): \n",
    "#     la.append(top_right['powered_within_outage'].values[0][i][1])\n",
    "#     lo.append(top_right['powered_within_outage'].values[0][i][0])\n",
    "# len(la)\n",
    "# sns.scatterplot(x=lo, y=la, label='powered')\n",
    "# sns.scatterplot(x=top_right['longitude'].values[0], y=top_right['latitude'].values[0], label='outage')\n",
    "# plt.title('Top Right Point: Likely More than One Concurrent Outage')\n",
    "# plt.xlabel('longitude')\n",
    "# plt.ylabel('latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "c = clustered.groupby('cluster').count()\n",
    "pair_index = c[c['time'] == 2].index\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for clusters of 2: \n",
    "pairs = clustered[clustered['cluster'].isin(pair_index)]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.location_longitude, pairs.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = clustered[clustered['cluster'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "#only one of the columns had all trios within 550 m so I took the average to see that 6/10 are below an average distance of 550\n",
    "trios_distances['avg'] = (trios_distances['2->3'] + trios_distances['1->2'] + trios_distances['3->1'])/3\n",
    "trios_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_for_3 = list(trios_distances['1->2'].values) + list(trios_distances['2->3'].values) + list(trios_distances['3->1'].values)\n",
    "plt.hist(dist_for_3, bins=20)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('STDBSCAN: Distances between sensors that are clustered as trios')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trios_distances['avg'], bins=15)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('STDBSCAN: Average distance between sensors that are clustered as trios')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('STDBSCAN: Distances between sensors that are clustered as pairs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's caclulate the percentage to the left of the red line for each of the graphs:\n",
    "#percentage under the cutoff for pairs: \n",
    "len(distances[distances < 550])/len(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage under the cutoff for trios: \n",
    "dist_for_3 = np.array(dist_for_3)\n",
    "len(dist_for_3[dist_for_3 < 550])/len(dist_for_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage under the cutoff for average trio distance: \n",
    "len(trios_distances[trios_distances['avg'] < 550]['avg'])/len(trios_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now let's do this same analysis with the logical grid distance as our distance metric \n",
    "logical = pd.read_csv('/Users/emilypaszkiewicz17/gridwatch-data-analysis/grid_distance.csv')\n",
    "logical = logical.reset_index()\n",
    "logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_logic(lst):\n",
    "#     logic = []\n",
    "#     index=0\n",
    "#     for p1 in lst:\n",
    "#         nest = []\n",
    "#         for p2 in lst: \n",
    "#             if list(logical[(logical['level_0'] == p1) & (logical['level_1'] == p2)]['level_0'].values) != [] :\n",
    "#                 nest.append(logical[(logical['level_0'] == p1) & (logical['level_1'] == p2)]['logical_grid_distance'].values[0])\n",
    "#         logic.append(nest)\n",
    "#         index += 1 \n",
    "#     return logic \n",
    "\n",
    "# def avg_grid(lst):\n",
    "#     avg = []\n",
    "#     for i in lst: \n",
    "#         avg.append(np.average(i))\n",
    "#     return np.average(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#be ware this takes forever to run!\n",
    "#month_clust['avg_logical_dist'] = (np.vectorize(retrieve_logic)(month_clust['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ids = month_clust[month_clust['cluster_size'] == 2]\n",
    "three_ids =  month_clust[month_clust['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(two_ids[two_ids['logical_distance'] ==1])/len(two_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dist_for_3 = list(three_ids['log_dist_1']) + list(three_ids['log_dist_2']) + list(three_ids['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attempt at plotting time distribution/range v num sensors for each cluster \n",
    "for i in range(len(month_clust)): \n",
    "    sns.distplot(month_clust['time'].values[i])\n",
    "plt.title('STDBSCAN Clustering Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIMODAL DIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clust_sizes = pd.DataFrame(month_clust.groupby('cluster_size')['ids'].nunique()).reset_index()\n",
    "clust_sizes\n",
    "sns.barplot(x='cluster_size', y='ids', data=clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}