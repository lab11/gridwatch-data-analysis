{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "import pyproj\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to initiate the STDBSCAN class \n",
    "\n",
    "class STDBSCAN(object):\n",
    "\n",
    "    def __init__(self, col_lat, col_lon, col_time, spatial_threshold=500.0, \n",
    "                 temporal_threshold=60.0, min_neighbors=15):\n",
    "        \"\"\"\n",
    "        Python st-dbscan implementation.\n",
    "        :param col_lat: Latitude column name;\n",
    "        :param col_lon:  Longitude column name;\n",
    "        :param col_time: Date time column name;\n",
    "        :param spatial_threshold: Maximum geographical coordinate (spatial)\n",
    "             distance value (meters);\n",
    "        :param temporal_threshold: Maximum non-spatial distance value (seconds);\n",
    "        :param min_neighbors: Minimum number of points within Eps1 and Eps2\n",
    "             distance;\n",
    "        \"\"\"\n",
    "        self.col_lat = col_lat\n",
    "        self.col_lon = col_lon\n",
    "        self.col_time = col_time\n",
    "        self.spatial_threshold = spatial_threshold\n",
    "        self.temporal_threshold = temporal_threshold\n",
    "        self.min_neighbors = min_neighbors\n",
    "\n",
    "    def projection(self, df, p1_str='epsg:4326', p2_str='epsg:3395'):\n",
    "        \"\"\"\n",
    "        Cython wrapper to converts from geographic (longitude,latitude)\n",
    "        to native map projection (x,y) coordinates. It needs to select the\n",
    "        right epsg. Values of x and y are given in meters\n",
    "        \"\"\"\n",
    "        p1 = pyproj.Proj(init=p1_str)\n",
    "        p2 = pyproj.Proj(init=p2_str)\n",
    "        lon = df[self.col_lon].values\n",
    "        lat = df[self.col_lat].values\n",
    "        x1, y1 = p1(lon, lat)\n",
    "        x2, y2 = pyproj.transform(p1, p2, x1, y1, radians=True)\n",
    "        df[self.col_lon] = x2\n",
    "        df[self.col_lat] = y2\n",
    "\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def _retrieve_neighbors(self, index_center, matrix):\n",
    "\n",
    "        center_point = matrix[index_center, :]\n",
    "\n",
    "        # filter by time\n",
    "        min_time = center_point[2] - timedelta(seconds=self.temporal_threshold)\n",
    "        max_time = center_point[2] + timedelta(seconds=self.temporal_threshold)\n",
    "        matrix = matrix[(matrix[:, 2] >= min_time) &\n",
    "                        (matrix[:, 2] <= max_time), :]\n",
    "        # filter by distance\n",
    "        tmp = (matrix[:, 0]-center_point[0])*(matrix[:, 0]-center_point[0]) + \\\n",
    "            (matrix[:, 1]-center_point[1])*(matrix[:, 1]-center_point[1])\n",
    "        neigborhood = matrix[tmp <= (\n",
    "            self.spatial_threshold*self.spatial_threshold), 4].tolist()\n",
    "        neigborhood.remove(index_center)\n",
    "\n",
    "        return neigborhood\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            df={o1,o2,...,on} Set of objects;\n",
    "        OUTPUT:\n",
    "            C = {c1,c2,...,ck} Set of clusters\n",
    "        \"\"\"\n",
    "        cluster_label = 0\n",
    "        noise = -1\n",
    "        unmarked = 777777\n",
    "        stack = []\n",
    "\n",
    "        # initial setup\n",
    "        df = df[[self.col_lon, self.col_lat, self.col_time]]\n",
    "        df = df.assign(cluster=unmarked)\n",
    "        df['index'] = range(df.shape[0])\n",
    "        matrix = df.values\n",
    "        df.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "        # for each point in database\n",
    "        for index in range(matrix.shape[0]):\n",
    "            if matrix[index, 3] == unmarked:\n",
    "                neighborhood = self._retrieve_neighbors(index, matrix)\n",
    "\n",
    "                if len(neighborhood) < self.min_neighbors:\n",
    "                    matrix[index, 3] = noise\n",
    "                else:  # found a core point\n",
    "                    cluster_label += 1\n",
    "                    # assign a label to core point\n",
    "                    matrix[index, 3] = cluster_label\n",
    "\n",
    "                    # assign core's label to its neighborhood\n",
    "                    for neig_index in neighborhood:\n",
    "                        matrix[neig_index, 3] = cluster_label\n",
    "                        stack.append(neig_index)  # append neighbors to stack\n",
    "\n",
    "                    # find new neighbors from core point neighborhood\n",
    "                    while len(stack) > 0:\n",
    "                        current_point_index = stack.pop()\n",
    "                        new_neighborhood = \\\n",
    "                            self._retrieve_neighbors(current_point_index,\n",
    "                                                     matrix)\n",
    "\n",
    "                        # current_point is a new core\n",
    "                        if len(new_neighborhood) >= self.min_neighbors:\n",
    "                            for neig_index in new_neighborhood:\n",
    "                                neig_cluster = matrix[neig_index, 3]\n",
    "                                if any([neig_cluster == noise,\n",
    "                                        neig_cluster == unmarked]):\n",
    "                                    matrix[neig_index, 3] = cluster_label\n",
    "                                    stack.append(neig_index)\n",
    "\n",
    "        df['cluster'] = matrix[:, 3]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def plot_clusters(df, output_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    labels = df['cluster'].values\n",
    "    X = df[['longitude', 'latitude']].values\n",
    "\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('ST-DSCAN: #n of clusters {}'.format(len(unique_labels)))\n",
    "    plt.show()\n",
    "    # plt.savefig(output_name)\n",
    "\n",
    "\n",
    "def test_time(df):\n",
    "    '''\n",
    "    transfrom the lon and lat to x and y\n",
    "    need to select the right epsg\n",
    "    I don't the true epsg of sample, but get the same result by using \n",
    "    epsg:4326 and epsg:32635\n",
    "    '''\n",
    "    st_dbscan = STDBSCAN(col_lat='location_latitude', col_lon='location_longitude',\n",
    "                         col_time='time', spatial_threshold=500,\n",
    "                         temporal_threshold=60, min_neighbors=2)\n",
    "    #df = st_dbscan.projection(df, p1_str='epsg:4326', p2_str='epsg:32630')\n",
    "    result_t180 = st_dbscan.run(df)\n",
    "    return result_t180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transition points of the outages \n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages['time'] = outages['outage_time'].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "outages['time'] = pd.to_datetime(outages['time'], infer_datetime_format=True)\n",
    "outages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.DataFrame(test_time(outages))\n",
    "    print(pd.value_counts(df['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = pd.DataFrame(test_time(outages))\n",
    "clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "clustered['time'] = clustered['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "month_clust = pd.DataFrame(clustered.groupby(['cluster'])['time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = clustered.groupby(['cluster'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = clustered.groupby(['cluster'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['time_range'], bins=30)\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Times in a Cluster')\n",
    "plt.xlabel('Time Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['lat_range'])\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Latitude in a Cluster')\n",
    "plt.xlabel('Latitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['long_range'])\n",
    "plt.title('STDBSCAN: A Distribution for the Range of Longitude in a Cluster')\n",
    "plt.xlabel('Longitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all of the data from July 2018 (not just outage transition points!) \n",
    "#only use this for SAIFI \n",
    "pw = pd.read_parquet('part-00000-602cb425-c6be-40be-8024-aeb92fcb4315-c000.gz.parquet')\n",
    "pw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calculate SAIFI \n",
    "st_SAIFI_num = len(clustered[clustered['cluster'] != -1])\n",
    "st_SAIFI_denom = len(pw['core_id'].unique())*(len(month_clust))\n",
    "st_SAIFI = st_SAIFI_num/st_SAIFI_denom\n",
    "st_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try it on 2 days of data to do the analysis that I was performing in evaluating_clustering_algorithm\n",
    "days = outages[outages['outage_time'] <= min(outages['outage_time'])+172800]\n",
    "days = pd.DataFrame(test_time(days))\n",
    "days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = days.rename(columns={'cluster': 'labels'})\n",
    "day_a = days[days['labels'] == 0]\n",
    "day_b = days[days['labels'] == 1]\n",
    "day_c = days[days['labels'] == 2]\n",
    "day_d = days[days['labels'] == 3]\n",
    "day_e = days[days['labels'] == 4]\n",
    "unlabeled = days[days['labels'] == -1]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(unlabeled['location_longitude'], unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_b['location_longitude'], day_b['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_c['location_longitude'], day_c['location_latitude'], label='cluster 2')\n",
    "plt.scatter(day_d['location_longitude'], day_d['location_latitude'], label='cluster 3')\n",
    "plt.scatter(day_e['location_longitude'], day_e['location_latitude'], label='cluster 4')\n",
    "plt.title('STDBSCAN Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlim(left, right)\n",
    "plt.ylim(top, bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "#days['time'] = days['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "days_clust = pd.DataFrame(days.groupby(['labels'])['time'].apply(lambda x: x.tolist())).rename(columns={'time': 'outage_times'})\n",
    "days_clust['latitude'] = days.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust['longitude'] = days.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust = days_clust.iloc[1:]\n",
    "\n",
    "days_clust['time_range'] = (np.vectorize(find_range)(days_clust['outage_times']))\n",
    "days_clust['lat_range'] = (np.vectorize(find_range)(days_clust['latitude']))\n",
    "days_clust['long_range'] = (np.vectorize(find_range)(days_clust['longitude']))\n",
    "\n",
    "days_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_clust['time'].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(days_clust['outage_times'].values[3], label='cluster_4')\n",
    "sns.distplot([1530449581, 1530449581, 1530449582], label='cluster_3')\n",
    "sns.distplot(days_clust['outage_times'].values[1], label='cluster_2')\n",
    "sns.distplot(days_clust['outage_times'].values[0], label='cluster_1')\n",
    "plt.legend()\n",
    "plt.title('STDBSCAN Clustering Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}