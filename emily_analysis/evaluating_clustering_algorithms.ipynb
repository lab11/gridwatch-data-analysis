{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time \n",
    "import datetime\n",
    "from datetime import timezone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import metrics \n",
    "import re\n",
    "import pylab\n",
    "from scipy.stats import norm\n",
    "import geopandas as gpd \n",
    "import shapely.geometry as geometry\n",
    "import shapely.ops as ops \n",
    "from functools import partial \n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#these are the transition points of the outages \n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all of the data from July 2018 (not just outage transition points!) \n",
    "#only use this for SAIFI \n",
    "pw = pd.read_parquet('part-00000-602cb425-c6be-40be-8024-aeb92fcb4315-c000.gz.parquet').drop(['product_id', 'millis', 'last_plug_millis', 'last_unplug_millis'], axis=1)\n",
    "pw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN(time) on a month's worth of data \n",
    "month_out = outages\n",
    "month_out['z'] = 0\n",
    "X=month_out[['outage_time', 'z']]\n",
    "out_cluster = StandardScaler().fit_transform(X)\n",
    "db = DBSCAN(eps=0.0001, algorithm='ball_tree').fit(out_cluster)\n",
    "labels = db.labels_\n",
    "mo_noise = list(labels).count(-1)\n",
    "mo_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "mo_noise, mo_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_out['labels'] =labels \n",
    "month_out.head()\n",
    "month_clust = pd.DataFrame(month_out.groupby(['labels'])['outage_time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = month_out.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = month_out.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['outage_time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#adjust bin size as well as the x and y bounds of the graph \n",
    "#this is a histogram of the range of times for each cluster \n",
    "plt.hist(month_clust['time_range'])\n",
    "plt.title('DBSCAN(time): A Distribution for the Range of Times in a Cluster')\n",
    "plt.xlabel('Time Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['lat_range'])\n",
    "plt.title('DBSCAN(time): A Distribution for the Range of Latitude in a Cluster')\n",
    "plt.xlabel('Latitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(month_clust['long_range'])\n",
    "plt.title('DBSCAN(time): A Distribution for the Range of Longitude in a Cluster')\n",
    "plt.xlabel('Longitude Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN(time + location) on a month's worth of data \n",
    "#was going to try to scale it, but first understand why this doesn't work ! \n",
    "Y=month_out[['outage_time', 'location_latitude', 'location_longitude']]\n",
    "out_cluster = StandardScaler().fit_transform(Y)\n",
    "db = DBSCAN(eps=0.1, algorithm='ball_tree').fit(out_cluster)\n",
    "labels = db.labels_\n",
    "mo_noise = list(labels).count(-1)\n",
    "mo_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "mo_noise, mo_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#used for SAIDI and SAIFI calculations \n",
    "pw_df = pw[pw['time'] < datetime.datetime(2018, 7, 3)]\n",
    "len(pw_df), len(outages), len(pw_df[pw_df['is_powered'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OPTICS clustering only time \n",
    "days = outages[outages['outage_time'] <= min(outages['outage_time'])+172800]\n",
    "days['zeros'] = 0\n",
    "out_cluster = days[['zeros', 'outage_time']]\n",
    "out_cluster = StandardScaler().fit_transform(out_cluster)\n",
    "optics = OPTICS(max_eps=0.7, algorithm='ball_tree').fit(out_cluster)\n",
    "labels = optics.labels_\n",
    "n_noise = list(labels).count(-1)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "days['labels'] = labels\n",
    "day_a = days[days['labels'] == 0]\n",
    "day_b = days[days['labels'] == 1]\n",
    "day_c = days[days['labels'] == 2]\n",
    "day_d = days[days['labels'] == 3]\n",
    "day_e = days[days['labels'] == 4]\n",
    "unlabeled = days[days['labels'] == -1]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(unlabeled['location_longitude'], unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_a['location_longitude'], day_a['location_latitude'], label='cluster 0')\n",
    "plt.scatter(day_b['location_longitude'], day_b['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_c['location_longitude'], day_c['location_latitude'], label='cluster 2')\n",
    "plt.scatter(day_d['location_longitude'], day_d['location_latitude'], label='cluster 3')\n",
    "plt.scatter(day_e['location_longitude'], day_e['location_latitude'], label='cluster 4')\n",
    "plt.title('Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "left, right = plt.xlim()\n",
    "top, bottom = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_a = day_a['outage_time'].values[0]\n",
    "for i in day_a['outage_time'].values[1:]: \n",
    "    print((time_a - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_b = day_b['outage_time'].values[0]\n",
    "for i in day_b['outage_time'].values[1:]: \n",
    "    print((time_b - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_c = day_c['outage_time'].values[0]\n",
    "for i in day_c['outage_time'].values[1:]: \n",
    "    print((time_c - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_SAIFI_num = len(days[days['labels'] != -1])\n",
    "optics_SAIFI_denom = len(pw_df['core_id'].unique())*(days['labels'].nunique()-1)\n",
    "optics_SAIFI = optics_SAIFI_num/optics_SAIFI_denom\n",
    "optics_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN clustering time + location \n",
    "out_df = outages[['location_latitude', 'location_longitude', 'outage_time']]\n",
    "day = out_df[out_df['outage_time'] <= min(out_df['outage_time'])+172800]\n",
    "out_cluster = StandardScaler().fit_transform(day)\n",
    "out_cluster = pd.DataFrame(out_cluster, columns=['outage_time', 'location_latitude', 'location_longitude'])\n",
    "out_cluster['outage_time'] = out_cluster['outage_time']*100\n",
    "recluster = out_cluster \n",
    "db = DBSCAN(eps=6, algorithm='ball_tree').fit(recluster)\n",
    "labels = db.labels_\n",
    "n_noise_ = list(labels).count(-1)\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_, n_clusters_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "day['labels'] = labels\n",
    "day_0 = day[day['labels'] == 0]\n",
    "day_1 = day[day['labels'] == 1]\n",
    "day_2 = day[day['labels'] == 2]\n",
    "day_3 = day[day['labels'] == 3]\n",
    "day_4 = day[day['labels'] == 4]\n",
    "day_unlabeled = day[day['labels'] == -1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(day_unlabeled['location_longitude'], day_unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_0['location_longitude'], day_0['location_latitude'], label='cluster 0')\n",
    "plt.scatter(day_1['location_longitude'], day_1['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_2['location_longitude'], day_2['location_latitude'], label='cluster 2')\n",
    "plt.scatter(day_3['location_longitude'], day_3['location_latitude'], label='cluster 3')\n",
    "plt.scatter(day_4['location_longitude'], day_4['location_latitude'], label='cluster 4')\n",
    "plt.title('Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "left, right = plt.xlim()\n",
    "top, bottom = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 0\n",
    "time0 = day[day['labels'] == 0]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 0]['outage_time'].values[1:]: \n",
    "    print((time0 - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 1\n",
    "time1 = day[day['labels'] == 1]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 1]['outage_time'].values[1:]: \n",
    "    print((time1 - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 2\n",
    "time2 = day[day['labels'] == 2]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 2]['outage_time'].values[1:]: \n",
    "    print((time2 - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 3\n",
    "time3 = day[day['labels'] == 3]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 3]['outage_time'].values[1:]: \n",
    "    print((time3 - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise = day[day['labels'] == -1]\n",
    "count = len(day[day['labels'] == -1]['outage_time'].values[1:])\n",
    "noise_dist = [0]\n",
    "for i in day[day['labels'] == -1]['outage_time'].values[1:]: \n",
    "    noise_dist.append(day[day['labels'] == -1]['outage_time'].values[0] - i)\n",
    "noise['noise_dist'] = noise_dist\n",
    "noise_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now let's plot noise points that have very little distance in time \n",
    "ex = noise[noise['noise_dist'] == 105081]\n",
    "plt.scatter(ex['location_longitude'], ex['location_latitude'])\n",
    "plt.xlim((left, right))\n",
    "plt.ylim((top, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_clusters = day.groupby('labels').mean()\n",
    "db_clusters['location_latitude'] = day.groupby('labels')['location_latitude'].apply(lambda x: x.to_list())\n",
    "db_clusters['location_longitude'] = day.groupby('labels')['location_longitude'].apply(lambda x: x.to_list())\n",
    "db_clusters['outage_times'] = day.groupby('labels')['outage_time'].apply(lambda x: x.to_list())\n",
    "db_clusters['stddev'] = db_clusters['outage_times'].apply(lambda x: np.std(x))\n",
    "db_clusters\n",
    "#now eliminate the first row and try to plot norm.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(db_clusters['outage_times'].values[3], label='cluster_2')\n",
    "sns.distplot(db_clusters['outage_times'].values[2], label='cluster_1')\n",
    "sns.distplot(db_clusters['outage_times'].values[1], label='cluster_0')\n",
    "plt.legend()\n",
    "plt.title('DBSCAN cluster distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_outages = pd.read_parquet('part-00000-1a77f616-ace0-482c-9ad1-bdc53a8286bc-c000.gz.parquet')\n",
    "spark_outages.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading outages from the pw_finalized_with_string dataframe from outage_aggregator.py and doing some data cleaning \n",
    "spark_outages = spark_outages[spark_outages['cluster_size'] > 1]\n",
    "spark_day = spark_outages\n",
    "spark_day['outage_times'] = spark_day['outage_times'].apply(lambda x: re.findall('\\d+', x))\n",
    "spark_day['location'] = spark_day['location'].apply(lambda x: re.findall('\\d.\\d+', x))\n",
    "spark_day_exploded = spark_day.explode('outage_times')\n",
    "spark_day_exploded['outage_times'] = spark_day_exploded['outage_times'].apply(lambda x: int(x))\n",
    "unexploded = spark_day_exploded.groupby('outage_time')['outage_times'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')\n",
    "unexploded['location'] = spark_day.sort_values('outage_time')['location'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#more data cleaning \n",
    "explode_loc = unexploded.explode('location')\n",
    "explode_loc['location'] = explode_loc['location'].apply(lambda x: float(x))\n",
    "lat = explode_loc[explode_loc['location'] > 1]\n",
    "long = explode_loc[explode_loc['location'] < 1]\n",
    "lat['latitude'] = lat['location']\n",
    "long['longitude'] = long['location']*(-1)\n",
    "lat = lat.groupby('outage_time')['latitude'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')\n",
    "lat['longitude'] = long.groupby('outage_time')['longitude'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')['longitude']\n",
    "lat['cluster_size'] = spark_day.sort_values('outage_time')['cluster_size'].values\n",
    "lat['outage_times_stddev'] = spark_day.sort_values('outage_time')['outage_times_stddev'].values\n",
    "lat['range'] = spark_day.sort_values('outage_time')['outage_times_range'].values\n",
    "lat['outage_times'] = unexploded.sort_values('outage_time')['outage_times']\n",
    "spark_day = lat\n",
    "spark_day['min_time'] = spark_day['outage_times'].apply(lambda x: min(x))\n",
    "spark_day['max_time'] = spark_day['outage_times'].apply(lambda x: max(x))\n",
    "spark_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agglom_SAIFI_num = sum(spark_day['cluster_size'].values)\n",
    "agglom_SAIFI_denom = len(pw_df['core_id'].unique())*(len(spark_day))\n",
    "agglom_SAIFI = agglom_SAIFI_num/agglom_SAIFI_denom\n",
    "agglom_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='outage_time', y='cluster_size', data=spark_day_exploded)\n",
    "plt.title('Outage time vs. Cluster Size for Agglomerative Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(spark_day['outage_times'].values[6], label='cluster_6')\n",
    "sns.distplot(spark_day['outage_times'].values[5], label='cluster_5')\n",
    "sns.distplot(spark_day['outage_times'].values[4], label='cluster_4')\n",
    "sns.distplot(spark_day['outage_times'].values[3], label='cluster_3')\n",
    "sns.distplot(spark_day['outage_times'].values[2], label='cluster_2')\n",
    "sns.distplot(spark_day['outage_times'].values[1], label='cluster_1')\n",
    "sns.distplot([1530449581, 1530449581, 1530449582], label='cluster_0')\n",
    "plt.legend()\n",
    "plt.title('Agglomerative Time Clustering Distributions')\n",
    "plt.xlabel('outage time distribution (sec)')\n",
    "plt.ylabel('relative cluster size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(db_clusters['outage_times'].values[3], label='cluster_2')\n",
    "sns.distplot(db_clusters['outage_times'].values[2], label='cluster_1')\n",
    "sns.distplot(db_clusters['outage_times'].values[1], label='cluster_0')\n",
    "plt.legend()\n",
    "plt.title('DBSCAN cluster distributions')\n",
    "plt.xlabel('outage time distribution (sec)')\n",
    "plt.ylabel('relative cluster size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(spark_day['longitude'].values[1], spark_day['latitude'].values[1], label='cluster 1')\n",
    "plt.scatter(spark_day['longitude'].values[2], spark_day['latitude'].values[2], label='cluster 2')\n",
    "plt.scatter(spark_day['longitude'].values[3], spark_day['latitude'].values[3], label='cluster 3')\n",
    "plt.scatter(spark_day['longitude'].values[4], spark_day['latitude'].values[4], label='cluster 4')\n",
    "plt.scatter(spark_day['longitude'].values[5], spark_day['latitude'].values[5], label='cluster 5')\n",
    "plt.scatter(spark_day['longitude'].values[6], spark_day['latitude'].values[6], label='cluster 6')\n",
    "plt.title('Agglomorative Time Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlim(left, right)\n",
    "plt.ylim(top, bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "# plt.scatter(day_unlabeled['location_longitude'], day_unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_0['location_longitude'], day_0['location_latitude'], label='cluster 0')\n",
    "plt.scatter(day_1['location_longitude'], day_1['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_2['location_longitude'], day_2['location_latitude'], label='cluster 2')\n",
    "#plt.scatter(day_3['location_longitude'], day_3['location_latitude'], label='cluster 3')\n",
    "#plt.scatter(day_4['location_longitude'], day_4['location_latitude'], label='cluster 4')\n",
    "plt.title('DBSCAN Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlim((left, right))\n",
    "plt.ylim((top, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(spark_outages['outage_times_range'], bins=30)\n",
    "plt.title('Agglomerative: A Distribution for the Range of Times in a Cluster')\n",
    "plt.xlabel('Time Range in a Cluster')\n",
    "plt.ylabel('Percentage of Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's compute the number of sensors within the convex hull of the outage \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage \n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = [[]]*len(spark_day)\n",
    "within_time_long = [[]]*len(spark_day)\n",
    "for p_index in range(len(powered)): \n",
    "    for o_index in range(len(spark_day)): \n",
    "        if powered['time'].values[p_index] >= spark_day['min_time'].values[o_index] and powered['time'].values[p_index] <= spark_day['max_time'].values[o_index]:\n",
    "            within_time_lat[o_index].append(powered['location_latitude'].values[p_index])\n",
    "            within_time_long[o_index].append(powered['location_longitude'].values[p_index])\n",
    "            \n",
    "spark_day['within_time_lat'] = within_time_lat\n",
    "spark_day['within_time_long'] = within_time_long\n",
    "spark_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = spark_day['latitude'].copy()\n",
    "update_long = spark_day['longitude'].copy()\n",
    "for i in range(len(spark_day)): \n",
    "    if spark_day.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = spark_day.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = spark_day.iloc[i]['longitude']*2\n",
    "\n",
    "spark_day['latitude'] = update_lat\n",
    "spark_day['longitude'] = update_long\n",
    "spark_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create geodataframes to calculate convex hull \n",
    "power = spark_day.copy()\n",
    "out = spark_day.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(spark_day)):\n",
    "    a = spark_day.iloc[i, :]['within_time_long']\n",
    "    b = spark_day.iloc[i, :]['within_time_lat']\n",
    "    c = spark_day.iloc[i, :]['longitude']\n",
    "    d = spark_day.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "spark_day['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "spark_day['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "spark_day['powered_poly'] = (np.vectorize(unique_coords)(spark_day['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "spark_day['outage_poly'] = (np.vectorize(unique_coords)(spark_day['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot size of outage vs. % out at within the convex hull \n",
    "\n",
    "def outage_size(outage_coords): \n",
    "    return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x='cluster_size', y='percent_pow_within_outage', data=out)\n",
    "plt.title('Agglomerative: Number of Sensors in Outage vs. Percent of Sensors Experiencing Outage Within the Convex Hull of the Outage')\n",
    "plt.xlabel('Number of Sensors in an Outage')\n",
    "plt.ylabel('Percentage of Sensors Powered within Convex Hull')\n",
    "\n",
    "left, right = plt.xlim()\n",
    "top, bottom = plt.ylim()\n",
    "#pick these points out and plot them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "top, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "t = spark_outages[spark_outages['cluster_size'] <= 3]\n",
    "explode_loc = t.explode('location')\n",
    "explode_loc['location'] = explode_loc['location'].apply(lambda x: float(x))\n",
    "lat = explode_loc[explode_loc['location'] > 1]\n",
    "long = explode_loc[explode_loc['location'] < 1]\n",
    "lat['latitude'] = lat['location']\n",
    "t = lat[['outage_time', 'outage_times', 'cluster_size', 'latitude']]\n",
    "t['longitude'] = long['location']*(-1)\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for sensors of cluster size 2 \n",
    "pairs = t[t['cluster_size'] == 2]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.longitude, pairs.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#dustances for sensors of cluster size 3 \n",
    "trios = t[t['cluster_size'] == 3]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.longitude, trios.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "#only one of the columns had all trios within 550 m so I took the average to see that 6/10 are below an average distance of 550\n",
    "trios_distances['avg'] = (trios_distances['2->3'] + trios_distances['1->2'] + trios_distances['3->1'])/3\n",
    "trios_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_for_3 = list(trios_distances['1->2'].values) + list(trios_distances['2->3'].values) + list(trios_distances['3->1'].values)\n",
    "plt.hist(dist_for_3, bins=20)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('Agglomerative: Distances between sensors that are clustered as trios')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trios_distances['avg'], bins=15)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('Agglomerative: Average distance between sensors that are clustered as trios')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances)\n",
    "plt.axvline(x=550, label='550m cutoff', c='r')\n",
    "plt.title('Agglomerative: Distances between sensors that are clustered as pairs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's caclulate the percentage to the left of the red line for each of the graphs:\n",
    "#percentage under the cutoff for pairs: \n",
    "len(distances[distances < 550])/len(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage under the cutoff for trios: \n",
    "dist_for_3 = np.array(dist_for_3)\n",
    "len(dist_for_3[dist_for_3 < 550])/len(dist_for_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage under the cutoff for average trio distance: \n",
    "len(trios_distances[trios_distances['avg'] < 550]['avg'])/len(trios_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using DBSCAN only with time \n",
    "out_df = outages[['outage_time', 'location_latitude', 'location_longitude']]\n",
    "day = out_df[out_df['outage_time'] <= min(out_df['outage_time'])+172800]\n",
    "day['zeros'] = 0\n",
    "out_cluster = StandardScaler().fit_transform(day[['outage_time', 'zeros']])\n",
    "db = DBSCAN(eps=.07, algorithm='ball_tree').fit(out_cluster)\n",
    "labels = db.labels_\n",
    "n_noise_ = list(labels).count(-1)\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_, n_clusters_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day['labels'] = labels\n",
    "day_0 = day[day['labels'] == 0]\n",
    "day_1 = day[day['labels'] == 1]\n",
    "day_2 = day[day['labels'] == 2]\n",
    "day_3 = day[day['labels'] == 3]\n",
    "day_4 = day[day['labels'] == 4]\n",
    "day_unlabeled = day[day['labels'] == -1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(day_unlabeled['location_longitude'], day_unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_0['location_longitude'], day_0['location_latitude'], label='cluster 0')\n",
    "plt.scatter(day_1['location_longitude'], day_1['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_2['location_longitude'], day_2['location_latitude'], label='cluster 2')\n",
    "plt.scatter(day_3['location_longitude'], day_3['location_latitude'], label='cluster 3')\n",
    "plt.scatter(day_4['location_longitude'], day_4['location_latitude'], label='cluster 4')\n",
    "plt.title('Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlim((left, right))\n",
    "plt.ylim((top, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 0\n",
    "time0 = day[day['labels'] == 0]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 0]['outage_time'].values[1:]: \n",
    "    print((time0 - i))\n",
    "time0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 1\n",
    "time1 = day[day['labels'] == 1]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 1]['outage_time'].values[1:]: \n",
    "    print((time1 - i))\n",
    "time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 2\n",
    "time2 = day[day['labels'] == 2]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 2]['outage_time'].values[1:]: \n",
    "    print((time2 - i))\n",
    "time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "time3 = day[day['labels'] == 3]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 3]['outage_time'].values[1:]: \n",
    "    print((time3 - i))\n",
    "time3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "time4 = day[day['labels'] == 4]['outage_time'].values[0]\n",
    "for i in day[day['labels'] == 4]['outage_time'].values[1:]: \n",
    "    print((time4 - i))\n",
    "time4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_SAIFI_num = len(day[day['labels'] != -1])\n",
    "db_SAIFI_denom = len(pw_df['core_id'].unique())*3\n",
    "db_SAIFI = db_SAIFI_num/db_SAIFI_denom\n",
    "db_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "#days['time'] = days['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "days_clust = pd.DataFrame(day.groupby(['labels'])['outage_time'].apply(lambda x: x.tolist())).rename(columns={'outage_time': 'outage_times'})\n",
    "days_clust['latitude'] = day.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust['longitude'] = day.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust = days_clust.iloc[1:]\n",
    "\n",
    "days_clust['time_range'] = (np.vectorize(find_range)(days_clust['outage_times']))\n",
    "days_clust['lat_range'] = (np.vectorize(find_range)(days_clust['latitude']))\n",
    "days_clust['long_range'] = (np.vectorize(find_range)(days_clust['longitude']))\n",
    "\n",
    "days_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}