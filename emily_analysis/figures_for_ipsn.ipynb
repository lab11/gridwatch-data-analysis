{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for IPSN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the notebook: \n",
    "\n",
    "1) Clustering and analysis for DB, Agglom, STDB which are ultimately stored in the dataframes: *db, agglom, stdb*\n",
    "\n",
    "2) Plots that Josh and Noah requested\n",
    "\n",
    "\n",
    "Disclaimers:\n",
    "- I plotted each clustering method separately instead of all compiled on one graph \n",
    "- Agglom logical distance hasn't been computed because the agglomerative csv I pulled doesn't have core_id's and I didn't want to mess with it \n",
    "- be wary about the order in which you run cells \n",
    "    - (the code is super repetitive since I pulled from a bunch of different notebooks, but I did my best to make it user friendly)\n",
    "- Plots 3 and 4 are not very smooth... hopefully it will improve with more data, and if not maybe Matt can help?\n",
    "    \n",
    "Let me know if you run into any problems! - Emily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install all of these libraries before beginning \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time \n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics \n",
    "import re\n",
    "import pylab\n",
    "from scipy.stats import norm\n",
    "import geopandas as gpd \n",
    "import shapely.geometry as geometry\n",
    "import shapely.ops as ops \n",
    "from functools import partial \n",
    "import pyproj\n",
    "import pyarrow\n",
    "from statistics import mode, mean\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for all Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#these are the transition points of the outages from July 2018 pulled from outage_aggregator.py\n",
    "def read_transition_data():\n",
    "    outages = pd.read_parquet('../analysis-figures/clustering-analysis/all_transitions.gz.parquet', engine='pyarrow')\n",
    "    out2 = outages.groupby(['core_id','outage_time'])['powered_sensors'].apply(lambda x: x.tolist()).values\n",
    "    outages = outages.groupby(['core_id','outage_time']).first().reset_index()\n",
    "    outages['powered_sensors'] = out2\n",
    "    return outages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical grid distance csv \n",
    "def read_logical_grid():\n",
    "    logical = pd.read_csv('../grid_distance.csv')\n",
    "    logical = logical.reset_index()\n",
    "    return logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exctract data from the dbscan result and do some bookkeeping on other columns\n",
    "def prep_cluster_data(month_out, time_label):\n",
    "    month_clust = pd.DataFrame(month_out.groupby(['labels'])[time_label].apply(lambda x: x.tolist()))\n",
    "    month_clust['latitude'] = month_out.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "    month_clust['longitude'] = month_out.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "    month_clust['powered_list'] = month_out.groupby(['labels'])['powered_sensors'].apply(lambda x: x.tolist()).values\n",
    "    month_clust['powered_list'] = month_clust['powered_list'].apply(lambda x: [item for sublist in x for item in sublist]).values\n",
    "    month_clust['powered_list'] = month_clust['powered_list'].apply(lambda x: list({v['core_id']:v for v in x}.values())).values\n",
    "    month_clust['num_powered'] = month_clust['powered_list'].apply(lambda x: len(x)).values\n",
    "    month_clust['sensors_reporting'] = month_out.groupby(['labels'])['sensors_reporting'].apply(lambda x: x.tolist()).values\n",
    "    month_clust['sensors_reporting'] = month_clust['sensors_reporting'].apply(lambda x: int(mean(x))).values \n",
    "    month_clust['ids'] = month_out.groupby(['labels'])['core_id'].apply(lambda x: x.tolist()).values\n",
    "    if time_label == 'outage_time':\n",
    "        other_time = 'restore_time'\n",
    "    else:\n",
    "        other_time = 'outage_time'\n",
    "    month_clust[other_time] = month_out.groupby(['labels'])[other_time].apply(lambda x: x.tolist()).values\n",
    "    month_clust = month_clust.iloc[1:]\n",
    "\n",
    "    def find_range(lst):\n",
    "        return max(lst) - min(lst)\n",
    "\n",
    "    month_clust['time_range'] = (np.vectorize(find_range)(month_clust[time_label]))\n",
    "    month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "    month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "    month_clust['cluster_size'] = month_out.groupby(['labels'])[time_label].count()\n",
    "    month_clust['min_time'] = month_clust[time_label].apply(lambda x: min(x))\n",
    "    month_clust['max_time'] = month_clust[time_label].apply(lambda x: max(x))\n",
    "\n",
    "    def remove_overlapping_ids(series):\n",
    "        indexes_to_remove = []\n",
    "        for index, sensor in enumerate(series[3]):\n",
    "            if sensor['core_id'] in series[6]:\n",
    "                indexes_to_remove.append(index)\n",
    "\n",
    "        for i in sorted(indexes_to_remove, reverse=True):\n",
    "            del series[3][i]\n",
    "\n",
    "        return series[3]\n",
    "\n",
    "    month_clust['powered_list'] = month_clust.apply(remove_overlapping_ids,axis=1).values\n",
    "    month_clust['num_powered'] = month_clust['powered_list'].apply(lambda x: len(x)).values\n",
    "    month_clust['within_time_lat'] = month_clust['powered_list'].apply(lambda x: list(v['location_latitude'] for v in x)).values\n",
    "    month_clust['within_time_long'] = month_clust['powered_list'].apply(lambda x: list(v['location_longitude'] for v in x)).values\n",
    "        \n",
    "    return month_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_convex_hull(month_clust, time_label):\n",
    "\n",
    "    # Prep the dataframe for CONVEX HULL Calculations\n",
    "    #for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "    update_lat = month_clust['latitude'].copy()\n",
    "    update_long = month_clust['longitude'].copy()\n",
    "    update_within_lat = month_clust['within_time_lat'].copy()\n",
    "    update_within_long = month_clust['within_time_long'].copy()\n",
    "\n",
    "    for i in range(len(month_clust)): \n",
    "        if len(month_clust.iloc[i]['within_time_lat']) == 0: \n",
    "            update_within_lat.values[i] = [0, 1, 2]\n",
    "            update_within_long.values[i] = [0, 1, 2] \n",
    "        if len(update_within_lat.values[i]) < 3: \n",
    "            update_within_lat.values[i] = month_clust.iloc[i]['within_time_lat']*3\n",
    "            update_within_long.values[i] = month_clust.iloc[i]['within_time_long']*3        \n",
    "        if month_clust.iloc[i]['cluster_size'] < 3: \n",
    "            update_lat.values[i] = month_clust.iloc[i]['latitude']*2\n",
    "            update_long.values[i] = month_clust.iloc[i]['longitude']*2\n",
    "\n",
    "    month_clust['latitude'] = update_lat\n",
    "    month_clust['longitude'] = update_long\n",
    "    month_clust['within_time_long'] = update_within_long\n",
    "    month_clust['within_time_lat'] = update_within_lat\n",
    "\n",
    "    #create geodataframes to calculate convex hull \n",
    "    power = month_clust.copy()\n",
    "    out = month_clust.copy()\n",
    "    powered_poly = []\n",
    "    outage_poly = []\n",
    "    for i in range(len(month_clust)):\n",
    "        a = month_clust.iloc[i, :]['within_time_long']\n",
    "        b = month_clust.iloc[i, :]['within_time_lat']\n",
    "        c = month_clust.iloc[i, :]['longitude']\n",
    "        d = month_clust.iloc[i, :]['latitude']\n",
    "        powered_poly.append(list(zip(a, b)))\n",
    "        outage_poly.append(list(zip(c, d)))\n",
    "\n",
    "    def unique_coords(coords):\n",
    "        return pd.Series(coords).unique()\n",
    "\n",
    "    power['powered_poly'] = powered_poly\n",
    "    out['powered_poly'] = powered_poly\n",
    "    month_clust['powered_poly'] = powered_poly\n",
    "    out['outage_poly'] = outage_poly\n",
    "    power['outage_poly'] = outage_poly\n",
    "    month_clust['outage_poly'] = outage_poly\n",
    "    crs = {'init', 'epsg:4326'}\n",
    "\n",
    "    powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "    power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "    outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "    out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "    power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "    out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "    month_clust['powered_poly'] = (np.vectorize(unique_coords)(month_clust['powered_poly']))\n",
    "    out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "    power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "    month_clust['outage_poly'] = (np.vectorize(unique_coords)(month_clust['outage_poly']))\n",
    "\n",
    "    power['convex_area_powered'] = power.convex_hull\n",
    "    out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "    #calculate the convex hull \n",
    "    def in_convex_hull(powered_coords, geom):\n",
    "    #takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "        in_convex_hull = []\n",
    "        for i in powered_coords: \n",
    "            if geom.convex_hull.contains(geometry.Point(i)):\n",
    "                in_convex_hull.append(i)\n",
    "        in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "        return in_convex_hull\n",
    "\n",
    "    in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "    out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "    def outage_size(outage_coords): \n",
    "        return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "    out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "    out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "    if time_label == 'outage_time':\n",
    "        other_time = 'restore_time'\n",
    "    else:\n",
    "        other_time = 'outage_time'\n",
    "    \n",
    "    db = out[[time_label,'ids','sensors_reporting','cluster_size','powered_size_within_outage_area','percent_pow_within_outage', other_time]]\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAIDI calculation functions \n",
    "\n",
    "#first read the lookup table for identifying restore labels \n",
    "lookup = pd.read_parquet('outage_lookup.gz.parquet')\n",
    "lookup['test_time'] = lookup['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "uplook = pd.DataFrame()\n",
    "for i in lookup['core_id'].unique(): \n",
    "    relevant = lookup[lookup['core_id'] == i]\n",
    "    relevant = relevant.sort_values('outage_time')\n",
    "    relevant['trans_#'] = range(len(relevant))\n",
    "    uplook = uplook.append(relevant)\n",
    "lookup = uplook\n",
    "lookup.head()\n",
    "\n",
    "\n",
    "#ignoring restore clusters \n",
    "def ignore_restore(df):\n",
    "    df['avg_out'] = df['outage_time'].apply(np.mean)\n",
    "    df['avg_restore'] = df['restore_time'].apply(np.mean)\n",
    "    df['std_out'] = df['outage_time'].apply(np.std)\n",
    "    df['std_restore'] = df['restore_time'].apply(np.std)\n",
    "    df['clust_saidi'] = (df['avg_restore'] - df['avg_out'])*df['cluster_size']\n",
    "    no_rest_SAIDI = np.sum(db_out['clust_saidi'])/mean(db_out['sensors_reporting'])\n",
    "    print(f'SAIDI when we ignore restore clustering: {no_rest_SAIDI}')\n",
    "    return df\n",
    "\n",
    "\n",
    "#no noise \n",
    "def restore_clusters_exploded(db_out, db_restore, lookup, out_or_restore, other):   \n",
    "    r_times = pd.DataFrame(db_restore.explode('restore_time')['restore_time'])\n",
    "    r_ids = pd.DataFrame(db_restore.explode('ids')['ids'])\n",
    "    o_times = pd.DataFrame(db_out.explode('outage_time')['outage_time'])\n",
    "    o_ids = pd.DataFrame(db_out.explode('ids')['ids'])\n",
    "    restore = r_times.combine_first(r_ids).reset_index()\n",
    "    out = o_times.combine_first(o_ids).reset_index()\n",
    "    restore_labels = []\n",
    "    lat = []\n",
    "    long = []\n",
    "    multi = 0 \n",
    "    for i in range(len(out)):\n",
    "        time, eyed = out.iloc[i][out_or_restore], out.iloc[i]['ids']\n",
    "        lookedup = lookup[(lookup[out_or_restore] == time) & (lookup['core_id'] == eyed) & (lookup['is_powered'] == False)]\n",
    "        lat.append(lookedup['location_latitude'].values[0])\n",
    "        long.append(lookedup['location_longitude'].values[0])\n",
    "        if len(lookedup) == 1: \n",
    "            rest_time = lookedup[other].values[0]\n",
    "            row = restore[(restore[other] == rest_time) & (restore['ids'] == eyed)]\n",
    "            if len(row) == 1:\n",
    "                restore_labels.append(row['labels'].values[0])\n",
    "            else: \n",
    "                restore_labels.append(-1)\n",
    "        elif len(lookedup) > 1: \n",
    "            multi += 1 \n",
    "            rest_time = lookedup[other].values[0]\n",
    "            row = restore[(restore[other] == rest_time) & (restore['ids'] == eyed)]\n",
    "            if len(row) == 1:\n",
    "                restore_labels.append(row['labels'].values[0])\n",
    "            else: \n",
    "                restore_labels.append(-1)\n",
    "        else: \n",
    "            print('problem: outage time not in the lookup table')\n",
    "    out['restore_time'] = db_out.explode('restore_time')['restore_time'].values\n",
    "    out['restore_labels'] = restore_labels\n",
    "    out['latitude'] = lat \n",
    "    out['longitude'] = long\n",
    "    out['sensors_reporting'] = db_out.explode('outage_time')['sensors_reporting'].values\n",
    "    return db_w_labels(out, 'restore_labels') \n",
    "\n",
    "\n",
    "def db_w_labels(df, o_r_labels):\n",
    "    percentage = []\n",
    "    noise = []\n",
    "    out_size = []\n",
    "    for i in df['labels'].unique(): \n",
    "        a_df = df[df['labels'] == i]\n",
    "        per = [len(a_df[o_r_labels].unique())]*len(a_df)\n",
    "        n = [(len(a_df[a_df[o_r_labels] == -1]))/(len(a_df))]*len(a_df)\n",
    "        o = [len(a_df)]*len(a_df)\n",
    "        percentage.append(per)\n",
    "        noise.append(n)\n",
    "        out_size.append(o)\n",
    "    percentage = [item for sublist in percentage for item in sublist]\n",
    "    noise = [item for sublist in noise for item in sublist]\n",
    "    out_size = [item for sublist in out_size for item in sublist]\n",
    "    df['restore_groups'] = percentage \n",
    "    df['%_noise'] = noise\n",
    "    df['out_size'] = out_size\n",
    "    rest_count = pd.DataFrame(df.groupby(o_r_labels).count()['labels'])\n",
    "    df = df.join(rest_count, on=o_r_labels, rsuffix='_').rename(columns={'labels_' : 'rest_size'})\n",
    "    avg_restore_time = pd.DataFrame(df.groupby('restore_labels')['restore_time'].apply(np.mean))\n",
    "    df = df.join(avg_restore_time, on='restore_labels', rsuffix='_').rename(columns={'restore_time_' : 'avg_rest_time'})\n",
    "    return n_noise_saidi(df)\n",
    "\n",
    "\n",
    "def avg_time_std_dur(df):\n",
    "    mean_outage = db_out_clusters.groupby('labels')['outage_time'].apply(mean)\n",
    "    mean_restore = db_out_clusters.groupby('labels')['restore_time'].apply(mean)\n",
    "    std_outage = db_out_clusters.groupby('labels')['outage_time'].apply(np.std)\n",
    "    std_restore = db_out_clusters.groupby('labels')['restore_time'].apply(np.std)\n",
    "    num_sens = df.groupby('labels').count()['ids']\n",
    "    \n",
    "    df = df.join(mean_outage, on='labels', rsuffix='_').rename(columns={'outage_time_': 'mean_outage_time'})\n",
    "    df = df.join(mean_restore, on='labels', rsuffix='_').rename(columns={'restore_time_': 'mean_restore_time'})\n",
    "    df = df.join(std_outage, on='labels', rsuffix='__').rename(columns={'outage_time__': 'outage_time_stdev'})\n",
    "    df = df.join(std_restore, on='labels', rsuffix='__').rename(columns={'restore_time__': 'restore_time_stdev'})\n",
    "    df = df.join(num_sens, on='labels', rsuffix='__').rename(columns={'ids__': 'num_sens_out'})\n",
    "    df['clust_saidi'] = (df['mean_restore_time'] - df['mean_outage_time'])*df['num_sens_out']\n",
    "    return df \n",
    "\n",
    "\n",
    "def n_noise_saidi(df):\n",
    "    no_noise_duration = avg_time_std_dur(df[df['restore_labels'] != -1])\n",
    "    noise_duration = avg_time_std_dur(df[df['restore_labels'] == -1])\n",
    "    no_noise_SAIDI = np.sum(no_noise_duration.groupby('labels').mean()['clust_saidi'])/ np.mean(no_noise_duration.groupby('labels').mean()['sensors_reporting'])\n",
    "    noise_SAIDI = np.sum(noise_duration.groupby('labels').mean()['clust_saidi'])/ np.mean(noise_duration.groupby('labels').mean()['sensors_reporting'])\n",
    "    print(f'SAIDI when we eliminate all noise: {noise_SAIDI}')\n",
    "    print(f'SAIDI when we only include noise points: {no_noise_SAIDI}')\n",
    "    return df\n",
    "\n",
    "\n",
    "#noise clustered with nearest time \n",
    "def closest_time(df):\n",
    "    noise = df[df['restore_labels'] == -1]\n",
    "    nearest = []\n",
    "    for i in noise['restore_time'].values:\n",
    "        nearest_label = pd.DataFrame(abs(avg_restore_time['restore_time'] - i)).sort_values('restore_time').index[0]\n",
    "        nearest.append(nearest_label)\n",
    "    noise['nearest_time_labels'] = nearest\n",
    "    noise = pd.DataFrame(noise['nearest_time_labels'])\n",
    "    updated_df = df.join(noise)\n",
    "    updated_df['restore_labels'] = updated_df['nearest_time_labels'].combine_first(df['restore_labels'])\n",
    "    avg_restore_times = pd.DataFrame(updated_df.groupby('restore_labels')['restore_time'].apply(np.mean))\n",
    "    closest_time_noise = updated_df.join(avg_restore_times, on='restore_labels', rsuffix='_').rename(columns={'restore_time_' : 'avg_rest_times'})\n",
    "    return avg_time_std_dur_noise(closest_time_noise)\n",
    "\n",
    "\n",
    "def avg_time_std_dur_noise(df):\n",
    "    mean_outage = df.groupby('labels')['outage_time'].apply(mean)\n",
    "    mean_restore = df.groupby('labels')['avg_rest_times'].apply(mean)\n",
    "    std_outage = df.groupby('labels')['outage_time'].apply(np.std)\n",
    "    std_restore = df.groupby('labels')['avg_rest_times'].apply(np.std)\n",
    "    \n",
    "    df = df.join(mean_outage, on='labels', rsuffix='_').rename(columns={'outage_time_': 'mean_outage_time'})\n",
    "    df = df.join(mean_restore, on='labels', rsuffix='_').rename(columns={'avg_rest_times_': 'mean_restore_time'})\n",
    "    df = df.join(std_outage, on='labels', rsuffix='__').rename(columns={'outage_time__': 'outage_time_stdev'})\n",
    "    df = df.join(std_restore, on='labels', rsuffix='__').rename(columns={'avg_rest_times__': 'restore_time_stdev'})\n",
    "    df['clust_saidi'] = (df['mean_restore_time'] - df['mean_outage_time'])*df['out_size']\n",
    "    time_noise_SAIDI = np.sum(time_noise_duration.groupby('labels').mean()['clust_saidi'])/ np.mean(time_noise_duration.groupby('labels').mean()['sensors_reporting'])\n",
    "    print(f'SAIDI when noise restores are clustered with the nearest restore in time: {time_noise_SAIDI}')\n",
    "    return df \n",
    "\n",
    "\n",
    "#noise clustered with nearest distance in space \n",
    "\n",
    "def closest_dist(df):\n",
    "    noise = df[df['restore_labels'] == -1]\n",
    "    nearest = []\n",
    "    for n in range(len(noise)):\n",
    "        lat = noise.iloc[n]['latitude']\n",
    "        long = noise.iloc[n]['longitude']\n",
    "        a_time = noise.iloc[n]['restore_time']\n",
    "        calc_dist = pd.DataFrame(np.sqrt((df['latitude'] - lat)**2 + (df['longitude'] - long)**2)) \n",
    "        calc_dist['labels'] = db_out_clusters['labels']\n",
    "        nearest_labels = calc_dist[calc_dist[0] == calc_dist.sort_values(0)[0].values]['labels']        \n",
    "        nearest_dist_times = avg_restore_time[avg_restore_time.index.isin(nearest_labels)]\n",
    "        nearest_label = pd.DataFrame(abs(nearest_dist_times['restore_time'] - a_time)).sort_values('restore_time').index[0]\n",
    "        nearest.append(nearest_label)\n",
    "    noise['nearest_dist_labels'] = nearest\n",
    "    noise = pd.DataFrame(noise['nearest_dist_labels'])\n",
    "    updated_df = df.join(noise)\n",
    "    updated_df['restore_labels'] = updated_df['nearest_dist_labels'].combine_first(df['restore_labels'])\n",
    "    avg_restore_times = pd.DataFrame(updated_df.groupby('restore_labels')['restore_time'].apply(np.mean))\n",
    "    updated_df = updated_df.join(avg_restore_times, on='restore_labels', rsuffix='_').rename(columns={'restore_time_' : 'avg_rest_times'})\n",
    "    dist_noise_duration = avg_time_std_dur_noise(updated_df)\n",
    "    dist_noise_SAIDI = np.sum(dist_noise_duration.groupby('labels').mean()['clust_saidi'])/ np.mean(time_noise_duration.groupby('labels').mean()['sensors_reporting'])\n",
    "    #note: this also prints out the nearest time SAIDI too \n",
    "    print(f'SAIDI when we place noise in the restore cluster with the closest distance and time:{dist_noise_SAIDI}')\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN(time only): Clustering and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DBSCAN(time) on a month's worth of data \n",
    "def run_dbscan(outages, time_label):\n",
    "    month_out = outages\n",
    "    month_out['z'] = 0\n",
    "    \n",
    "    # Prep the DBSCAN Columns - no need to normalization, eps in seconds\n",
    "    X=month_out[[time_label, 'z']]\n",
    "    out_cluster = X.values\n",
    "    \n",
    "    # Actually run dbscan\n",
    "    db = DBSCAN(eps=90, algorithm='ball_tree', min_samples=2).fit(out_cluster)\n",
    "    labels = db.labels_\n",
    "    no_noise = list(labels).count(-1)\n",
    "    no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    month_out['labels'] = labels\n",
    "    month_out['core_id'] = outages['core_id']\n",
    "    \n",
    "    return month_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_db(time_label):\n",
    "    data = read_transition_data()\n",
    "    clustered_outages = run_dbscan(data, time_label)\n",
    "    dbscan_formatted_clusters = prep_cluster_data(clustered_outages, time_label)\n",
    "    dbscan_outages = run_convex_hull(dbscan_formatted_clusters, time_label)\n",
    "    return dbscan_outages\n",
    "    \n",
    "db_out = cluster_db('outage_time')\n",
    "db_restore = cluster_db('restore_time')\n",
    "ignore_restore_clust = ignore_restore(db_out)\n",
    "db_clusters = restore_clusters_exploded(db_out, db_restore, lookup, 'outage_time', 'restore_time')\n",
    "avg_restore_time = pd.DataFrame(db_out_clusters.groupby('restore_labels')['restore_time'].apply(np.mean))\n",
    "db_nearest_time = closest_time(db_out_clusters)\n",
    "closest_dist_noise = closest_dist(db_out_clusters)\n",
    "closest_dist_noise\n",
    "db_nearest_time\n",
    "db_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_clusters_exploded(db_out, db_restore, lookup, 'outage_time', 'restore_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#percentage of noise (outage clusters that are not clustered in restore clusters)\n",
    "len(db_out_clusters[db_out_clusters['restore_labels'] == -1])/len(db_out_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "db_out_clust_grouped = db_out_clusters.groupby('labels').first()\n",
    "sns.scatterplot(x=\"out_size\", y=\"%_noise\", data=db_out_clust_grouped, label='Outage Clusters')\n",
    "plt.title('Outage Cluster Size vs. Percent Noise in Restore Cluster')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(db_out_clust_grouped['restore_groups'])\n",
    "plt.title('The number of sub-groups restored in an outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's find out the percentage of noise that is contained in restore clusters\n",
    "rest_group_clust = db_out_clusters[db_out_clusters['restore_labels'] != -1]\n",
    "rest_group_clust = rest_group_clust.groupby('restore_labels').first()\n",
    "db_restores = db_restore.join(rest_group_clust['rest_size'], on='labels', rsuffix='_').fillna(0)\n",
    "#rest_size here is the number of restores that are classified also as outages \n",
    "db_restores['%_noise'] = (1- db_restores['rest_size']/db_restores['cluster_size'])*100\n",
    "np.mean(db_restores['%_noise'])\n",
    "# this is the percentage of sensors that are in a restore cluster, but are not contained in an outage cluster ^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=\"rest_size\", y=\"%_noise\", data=db_restores, label='Restore Clusters')\n",
    "plt.title('Restore Cluster Size vs. Percent Noise in Restore Cluster')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(db_out_clust_grouped['%_noise']*100, label='outage_clusters', histtype='step')\n",
    "plt.hist(db_restores['%_noise'], label='restore_clusters', histtype='step')\n",
    "plt.title('Histogram of %_noise in a cluster')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative: Cleaning and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_agglomerative_cluster_data():\n",
    "    #clusters = pd.read_parquet('../analysis-figures/clustering-analysis/agglomerative_clusters.gz.parquet',engine='pyarrow')\n",
    "    clusters = spark.read.parquet('../analysis-figures/clustering-analysis/agglomerative_clusters.gz.parquet')\n",
    "    return clusters.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_cluster_prep(month_out):\n",
    "    month_clust = month_out\n",
    "    month_clust['latitude'] = month_clust['location'].apply(lambda x: list(l['location_latitude'] for l in x)).values\n",
    "    month_clust['longitude'] = month_clust['location'].apply(lambda x: list(l['location_longitude'] for l in x)).values\n",
    "    month_clust['ids'] = month_clust['core_id']\n",
    "    month_clust['outage_time'] = month_clust['outage_times']\n",
    "    \n",
    "    def tuple_to_dict(power_list):\n",
    "        l = []\n",
    "        \n",
    "        if power_list is None:\n",
    "            return l\n",
    "        \n",
    "        for x in power_list:\n",
    "            l.append({'core_id':x[0],'location_latitude':x[1],'location_longitude':x[2]})\n",
    "        \n",
    "        return l\n",
    "    \n",
    "    \n",
    "    month_clust['powered_list'] = month_clust['powered_sensors'].apply(tuple_to_dict).values\n",
    "    month_clust['num_powered'] = month_clust['powered_list'].apply(lambda x: len(x)).values\n",
    "    month_clust = month_clust.iloc[1:]\n",
    "\n",
    "    def find_range(lst):\n",
    "        return max(lst) - min(lst)\n",
    "\n",
    "    month_clust['time_range'] = (np.vectorize(find_range)(month_clust['outage_time']))\n",
    "    month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "    month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "    month_clust['min_time'] = month_clust['outage_time'].apply(lambda x: min(x))\n",
    "    month_clust['max_time'] = month_clust['outage_time'].apply(lambda x: max(x))\n",
    "\n",
    "    def remove_overlapping_ids(series):\n",
    "        indexes_to_remove = []\n",
    "        for index, sensor in enumerate(series[3]):\n",
    "            if sensor['core_id'] in series[6]:\n",
    "                indexes_to_remove.append(index)\n",
    "\n",
    "        for i in sorted(indexes_to_remove, reverse=True):\n",
    "            del series[3][i]\n",
    "\n",
    "        return series[3]\n",
    "    \n",
    "    month_clust = month_clust[['outage_time','latitude','longitude','powered_list','num_powered','sensors_reporting','ids','time_range','lat_range','long_range','cluster_size','min_time','max_time']]\n",
    "\n",
    "    month_clust['powered_list'] = month_clust.apply(remove_overlapping_ids,axis=1).values\n",
    "    month_clust['num_powered'] = month_clust['powered_list'].apply(lambda x: len(x)).values\n",
    "    month_clust['within_time_lat'] = month_clust['powered_list'].apply(lambda x: list(v['location_latitude'] for v in x)).values\n",
    "    month_clust['within_time_long'] = month_clust['powered_list'].apply(lambda x: list(v['location_longitude'] for v in x)).values\n",
    "    return month_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_agglomerative_cluster_data()\n",
    "formatted_clusters = agglomerative_cluster_prep(data)\n",
    "agglomerative_outages = run_convex_hull(formatted_clusters)\n",
    "agglomerative_outages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STDBSCAN: Clustering and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to initiate the STDBSCAN class \n",
    "\n",
    "class STDBSCAN(object):\n",
    "\n",
    "    def __init__(self, col_lat, col_lon, col_time, spatial_threshold=500.0, \n",
    "                 temporal_threshold=60.0, min_neighbors=15):\n",
    "        \"\"\"\n",
    "        Python st-dbscan implementation.\n",
    "        :param col_lat: Latitude column name;\n",
    "        :param col_lon:  Longitude column name;\n",
    "        :param col_time: Date time column name;\n",
    "        :param spatial_threshold: Maximum geographical coordinate (spatial)\n",
    "             distance value (meters);\n",
    "        :param temporal_threshold: Maximum non-spatial distance value (seconds);\n",
    "        :param min_neighbors: Minimum number of points within Eps1 and Eps2\n",
    "             distance;\n",
    "        \"\"\"\n",
    "        self.col_lat = col_lat\n",
    "        self.col_lon = col_lon\n",
    "        self.col_time = col_time\n",
    "        self.spatial_threshold = spatial_threshold\n",
    "        self.temporal_threshold = temporal_threshold\n",
    "        self.min_neighbors = min_neighbors\n",
    "\n",
    "    def projection(self, df, p1_str='epsg:4326', p2_str='epsg:3395'):\n",
    "        \"\"\"\n",
    "        Cython wrapper to converts from geographic (longitude,latitude)\n",
    "        to native map projection (x,y) coordinates. It needs to select the\n",
    "        right epsg. Values of x and y are given in meters\n",
    "        \"\"\"\n",
    "        p1 = pyproj.Proj(init=p1_str)\n",
    "        p2 = pyproj.Proj(init=p2_str)\n",
    "        lon = df[self.col_lon].values\n",
    "        lat = df[self.col_lat].values\n",
    "        x1, y1 = p1(lon, lat)\n",
    "        x2, y2 = pyproj.transform(p1, p2, x1, y1, radians=True)\n",
    "        df[self.col_lon] = x2\n",
    "        df[self.col_lat] = y2\n",
    "\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def _retrieve_neighbors(self, index_center, matrix):\n",
    "\n",
    "        center_point = matrix[index_center, :]\n",
    "\n",
    "        # filter by time\n",
    "        min_time = center_point[2] - timedelta(seconds=self.temporal_threshold)\n",
    "        max_time = center_point[2] + timedelta(seconds=self.temporal_threshold)\n",
    "        matrix = matrix[(matrix[:, 2] >= min_time) &\n",
    "                        (matrix[:, 2] <= max_time), :]\n",
    "        # filter by distance\n",
    "        tmp = (matrix[:, 0]-center_point[0])*(matrix[:, 0]-center_point[0]) + \\\n",
    "            (matrix[:, 1]-center_point[1])*(matrix[:, 1]-center_point[1])\n",
    "        neigborhood = matrix[tmp <= (\n",
    "            self.spatial_threshold*self.spatial_threshold), 4].tolist()\n",
    "        neigborhood.remove(index_center)\n",
    "\n",
    "        return neigborhood\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            df={o1,o2,...,on} Set of objects;\n",
    "        OUTPUT:\n",
    "            C = {c1,c2,...,ck} Set of clusters\n",
    "        \"\"\"\n",
    "        cluster_label = 0\n",
    "        noise = -1\n",
    "        unmarked = 777777\n",
    "        stack = []\n",
    "\n",
    "        # initial setup\n",
    "        df = df[[self.col_lon, self.col_lat, self.col_time]]\n",
    "        df = df.assign(cluster=unmarked)\n",
    "        df['index'] = range(df.shape[0])\n",
    "        matrix = df.values\n",
    "        df.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "        # for each point in database\n",
    "        for index in range(matrix.shape[0]):\n",
    "            if matrix[index, 3] == unmarked:\n",
    "                neighborhood = self._retrieve_neighbors(index, matrix)\n",
    "\n",
    "                if len(neighborhood) < self.min_neighbors:\n",
    "                    matrix[index, 3] = noise\n",
    "                else:  # found a core point\n",
    "                    cluster_label += 1\n",
    "                    # assign a label to core point\n",
    "                    matrix[index, 3] = cluster_label\n",
    "\n",
    "                    # assign core's label to its neighborhood\n",
    "                    for neig_index in neighborhood:\n",
    "                        matrix[neig_index, 3] = cluster_label\n",
    "                        stack.append(neig_index)  # append neighbors to stack\n",
    "\n",
    "                    # find new neighbors from core point neighborhood\n",
    "                    while len(stack) > 0:\n",
    "                        current_point_index = stack.pop()\n",
    "                        new_neighborhood = \\\n",
    "                            self._retrieve_neighbors(current_point_index,\n",
    "                                                     matrix)\n",
    "\n",
    "                        # current_point is a new core\n",
    "                        if len(new_neighborhood) >= self.min_neighbors:\n",
    "                            for neig_index in new_neighborhood:\n",
    "                                neig_cluster = matrix[neig_index, 3]\n",
    "                                if any([neig_cluster == noise,\n",
    "                                        neig_cluster == unmarked]):\n",
    "                                    matrix[neig_index, 3] = cluster_label\n",
    "                                    stack.append(neig_index)\n",
    "\n",
    "        df['labels'] = matrix[:, 3]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where you actually adjust the parameters \n",
    "\n",
    "def test_time(df):\n",
    "    '''\n",
    "    transfrom the lon and lat to x and y\n",
    "    need to select the right epsg\n",
    "    I don't the true epsg of sample, but get the same result by using \n",
    "    epsg:4326 and epsg:32635\n",
    "    '''\n",
    "    st_dbscan = STDBSCAN(col_lat='location_latitude', col_lon='location_longitude',\n",
    "                         col_time='time', spatial_threshold=0.03,\n",
    "                         temporal_threshold=90, min_neighbors=1)\n",
    "    #df = st_dbscan.projection(df, p1_str='epsg:4326', p2_str='epsg:32630')\n",
    "    return st_dbscan.run(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stdbscan(outages):\n",
    "    month_out = pd.DataFrame(test_time(outages))\n",
    "    not_noise = month_out['labels'] != -1\n",
    "    month_out = month_out[not_noise]\n",
    "    month_out['core_id'] = outages['core_id']\n",
    "    month_out['sensors_reporting'] = outages['sensors_reporting']\n",
    "    month_out['powered_sensors'] = outages['powered_sensors']\n",
    "    month_out['outage_time'] = month_out['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "    return month_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_transition_data()\n",
    "clustered_outages = run_stdbscan(data)\n",
    "formatted_clusters = prep_cluster_data(clustered_outages)\n",
    "stdbscan_outages = run_convex_hull(formatted_clusters)\n",
    "stdbscan_outages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 1: Trimodal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "db_clust_sizes = pd.DataFrame(dbscan_outages.groupby('cluster_size').count()).reset_index()\n",
    "db_clust_sizes = db_clust_sizes[['cluster_size','ids']]\n",
    "db_clust_sizes.to_csv('../analysis-figures/clustering-analysis/db_cluster_counts.csv')\n",
    "sns.barplot(x='cluster_size', y='ids', data=db_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('DBSCAN: Cluster Size vs. Number of Clusters of this Size')\n",
    "db_clust_sizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGLOM\n",
    "agglom_clust_sizes = pd.DataFrame(agglomerative_outages.groupby('cluster_size').count()).reset_index()\n",
    "agglom_clust_sizes = agglom_clust_sizes[['cluster_size','ids']]\n",
    "agglom_clust_sizes.to_csv('../analysis-figures/clustering-analysis/agglom_cluster_counts.csv')\n",
    "sns.barplot(x='cluster_size', y='ids', data=agglom_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('Agglomerative: Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN \n",
    "stdb_clust_sizes = pd.DataFrame(stdbscan_outages.groupby('cluster_size')['ids'].count()).reset_index()\n",
    "stdb_clust_sizes = stdb_clust_sizes[['cluster_size','ids']]\n",
    "stdb_clust_sizes.to_csv('../analysis-figures/clustering-analysis/stdb_cluster_counts.csv')\n",
    "sns.barplot(x='cluster_size', y='ids', data=stdb_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('STDBSCAN: Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2a: Low Voltage Success (Euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='epsg:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='epsg:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_distances(outages):\n",
    "    #Get only clusters of 2\n",
    "    pair_index = outages['cluster_size'] == 2\n",
    "    pairs = outages[pair_index]\n",
    "    pairs['time'] = pairs['outage_time'].apply(lambda x: mean(x)).values\n",
    "    \n",
    "    def zip_locations(series):\n",
    "        l = []\n",
    "        for i in range(0,len(series[1])):\n",
    "            l.append({'latitude':series[1][i],'longitude':series[2][i]})\n",
    "            \n",
    "        return l\n",
    "    \n",
    "    #Combine locations into a single array then explode\n",
    "    pairs['location'] = pairs.apply(zip_locations,axis=1).values\n",
    "    pairs = pairs.explode('location')\n",
    "    pairs['latitude'] = pairs['location'].apply(lambda x: x['latitude']).values\n",
    "    pairs['longitude'] = pairs['location'].apply(lambda x: x['longitude']).values\n",
    "    pairs = pairs[0:5]\n",
    "    print(pairs)\n",
    " \n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        pairs, geometry=gpd.points_from_xy(pairs.latitude, pairs.longitude), crs={'init':'esri:4326'})\n",
    "    dist_1 = gpd.GeoSeries(gdf.groupby('time')['geometry'].first(), crs={'init':'aea'})\n",
    "    dist_2 = gpd.GeoSeries(gdf.groupby('time')['geometry'].last(), crs={'init':'aea'})\n",
    "    db_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "    return db_distances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pair_distances(dbscan_formatted_clusters)\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = month_out[month_out['labels'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "db_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "db_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "db_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#percentage under 550m for pairs: \n",
    "db_pair_percent_under_550= len(db_distances[db_distances < 550])/len(db_distances)\n",
    "\n",
    "#percentage under 550m for trios: \n",
    "db_dist_for_3 = list(db_trios_distances['1->2'].values) + list(db_trios_distances['2->3'].values) + list(db_trios_distances['3->1'].values)\n",
    "db_dist_for_3 = np.array(db_dist_for_3)\n",
    "db_trio_percent_under_550 = len(db_dist_for_3[db_dist_for_3 < 550])/len(db_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_550, db_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGLOMERATIVE EUCLIDEAN\n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "t = spark_outages[spark_outages['cluster_size'] <= 3]\n",
    "explode_loc = t.explode('location')\n",
    "explode_loc['location'] = explode_loc['location'].apply(lambda x: float(x))\n",
    "lat = explode_loc[explode_loc['location'] > 1]\n",
    "long = explode_loc[explode_loc['location'] < 1]\n",
    "lat['latitude'] = lat['location']\n",
    "t = lat[['outage_time', 'outage_times', 'cluster_size', 'latitude']]\n",
    "t['longitude'] = long['location']*(-1)\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for sensors of cluster size 2 \n",
    "pairs = t[t['cluster_size'] == 2]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.longitude, pairs.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "agglom_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#dustances for sensors of cluster size 3 \n",
    "trios = t[t['cluster_size'] == 3]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.longitude, trios.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "agglom_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "agglom_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "agglom_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "agglom_trios_distances\n",
    "\n",
    "#I would use agglom_trios_distances_list for measuring clustering success \n",
    "agglom_dist_for_3 = list(agglom_trios_distances['1->2'].values) + list(agglom_trios_distances['2->3'].values) + list(agglom_trios_distances['3->1'].values)\n",
    "\n",
    "#now let's caclulate the percentage within 550 m \n",
    "#percentage under the cutoff for pairs: \n",
    "agglom_pair_percent_under_550 = len(agglom_distances[agglom_distances < 550])/len(agglom_distances)\n",
    "\n",
    "#percentage under the cutoff for trios: \n",
    "agglom_dist_for_3 = np.array(agglom_dist_for_3)\n",
    "agglom_trio_percent_under_550 = len(agglom_dist_for_3[agglom_dist_for_3 < 550])/len(agglom_dist_for_3)\n",
    "\n",
    "agglom_pair_percent_under_550, agglom_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN Euclidean \n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "c = stdb_clustered.groupby('cluster').count()\n",
    "pair_index = c[c['time'] == 2].index\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for clusters of 2: \n",
    "pairs = stdb_clustered[stdb_clustered['cluster'].isin(pair_index)]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.location_longitude, pairs.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "stdb_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = stdb_clustered[stdb_clustered['cluster'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "stdb_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "stdb_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "stdb_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#percentage under 550m for pairs: \n",
    "stdb_pair_percent_under_550= len(stdb_distances[stdb_distances < 550])/len(stdb_distances)\n",
    "\n",
    "#percentage under 550m for trios: \n",
    "stdb_dist_for_3 = list(stdb_trios_distances['1->2'].values) + list(stdb_trios_distances['2->3'].values) + list(stdb_trios_distances['3->1'].values)\n",
    "stdb_dist_for_3 = np.array(stdb_dist_for_3)\n",
    "stdb_trio_percent_under_550 = len(stdb_dist_for_3[stdb_dist_for_3 < 550])/len(stdb_dist_for_3)\n",
    "\n",
    "stdb_pair_percent_under_550, stdb_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2b: Low Voltage Success (Logical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Logical Grid \n",
    "\n",
    "two_ids = db[db['cluster_size'] == 2]\n",
    "three_ids =  db[db['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3\n",
    "db_logical_pairs = two_ids \n",
    "db_logical_trios = three_ids\n",
    "two_ids\n",
    "\n",
    "\n",
    "#calculate the % of outage pairs that are under the same transformer \n",
    "db_pair_percent_under_same_transformer = len(db_logical_pairs[db_logical_pairs['logical_distance'] ==1])/len(db_logical_pairs)\n",
    "\n",
    "#calculate the % of outage trios that are under the same transformer \n",
    "log_dist_for_3 = list(db_logical_trios['log_dist_1']) + list(db_logical_trios['log_dist_2']) + list(db_logical_trios['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "db_trio_percent_under_same_transformer = len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_same_transformer, db_trio_percent_under_same_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglom Logical Grid \n",
    "\n",
    "#we have to wait for agglom to have core_id's in order to calculate logical grid dist \n",
    "#this needs to happen either by merging dataframes or by adding them in spark\n",
    "#once you have the core_id's you should be able to easily copy and paste the code for the DBSCANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN Logical Grid \n",
    "\n",
    "two_ids = stdb[stdb['cluster_size'] == 2]\n",
    "three_ids =  stdb[stdb['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3\n",
    "stdb_logical_pairs = two_ids \n",
    "stdb_logical_trios = three_ids\n",
    "\n",
    "\n",
    "#calculate the % of outage pairs that are under the same transformer \n",
    "stdb_pair_percent_under_same_transformer = len(stdb_logical_pairs[stdb_logical_pairs['logical_distance'] ==1])/len(stdb_logical_pairs)\n",
    "\n",
    "#calculate the % of outage trios that are under the same transformer \n",
    "log_dist_for_3 = list(stdb_logical_trios['log_dist_1']) + list(stdb_logical_trios['log_dist_2']) + list(stdb_logical_trios['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "stdb_trio_percent_under_same_transformer = len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)\n",
    "\n",
    "stdb_pair_percent_under_same_transformer, stdb_trio_percent_under_same_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3: Outage Size v. time variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "db['outage_time_stddev'] = dbscan_outages['outage_time'].apply(lambda x: np.std(x)).values\n",
    "db_clust_stddev = db.groupby('cluster_size')['outage_time_stddev'].apply(np.mean).reset_index()\n",
    "db_clust_stddev = db_clust_stddev[['cluster_size','outage_time_stddev']]\n",
    "db_clust_stddev.to_csv('../analysis-figures/clustering-analysis/db_time_stddev.csv')\n",
    "sns.lineplot(x='cluster_size', y='outage_time_stddev', data=db_clust_stddev, label='DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGGLOMERATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_outages['outage_time_stddev'] = agglomerative_outages['outage_time'].apply(lambda x: np.std(x)).values\n",
    "agglom_clust_stddev = pd.DataFrame(agglomerative_outages.groupby('cluster_size')['outage_time_stddev'].apply(np.mean)).reset_index()\n",
    "agglom_clust_stddev = agglom_clust_stddev[['cluster_size','outage_time_stddev']]\n",
    "agglom_clust_stddev.to_csv('../analysis-figures/clustering-analysis/agglom_time_stddev.csv')\n",
    "sns.lineplot(x='cluster_size', y='outage_time_stddev', data=agglom_clust_stddev, label='AGGLOMERATIVE')\n",
    "plt.title('Outage Size v. Average Time Range of Outage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdbscan_outages['outage_time_stddev'] = stdbscan_outages['outage_time'].apply(lambda x: np.std(x)).values\n",
    "stdb_clust_stddev = pd.DataFrame(stdbscan_outages.groupby('cluster_size')['outage_time_stddev'].apply(np.mean)).reset_index()\n",
    "stdb_clust_stddev = stdb_clust_stddev[['cluster_size','outage_time_stddev']]\n",
    "stdb_clust_stddev.to_csv('../analysis-figures/clustering-analysis/stdb_time_stddev.csv')\n",
    "sns.lineplot(x='cluster_size', y='outage_time_stddev', data=stdb_clust_stddev, label='STDBSCAN')\n",
    "plt.title('Outage Size v. Average Time Range of Outage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 4: Percent in Covex Hull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "db_convex_hull = dbscan_outages.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean).reset_index()\n",
    "db_convex_hull.to_csv('../analysis-figures/clustering-analysis/db_convex_hull.csv')\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=db_convex_hull, label='DBSCAN')\n",
    "plt.title('Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglom_convex_hull = agglomerative_outages.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean).reset_index()\n",
    "agglom_convex_hull.to_csv('../analysis-figures/clustering-analysis/agglom_convex_hull.csv')\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=agglom_convex_hull, label='AGGLOMERATIVE')\n",
    "plt.title('Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stdb_convex_hull = stdbscan_outages.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean).reset_index()\n",
    "stdb_convex_hull.to_csv('../analysis-figures/clustering-analysis/stdb_convex_hull.csv')\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=stdb_convex_hull, label='STDBSCAN')\n",
    "plt.title('Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 5: SAFI Calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAIFI Calculations are currently being calculated for the entire time period. Make sure to split the data into July, Aug, Sept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN \n",
    "db_SAIFI_num = sum(db['cluster_size'].values)\n",
    "db_SAIFI_denom = len(pw['core_id'].unique())*(len(db))\n",
    "db_SAIFI = db_SAIFI_num/db_SAIFI_denom\n",
    "db_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative \n",
    "agglom_SAIFI_num = sum(agglom['cluster_size'].values)\n",
    "agglom_SAIFI_denom = len(pw['core_id'].unique())*(len(agglom))\n",
    "agglom_SAIFI = agglom_SAIFI_num/agglom_SAIFI_denom\n",
    "agglom_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN\n",
    "stdb_SAIFI_num = sum(stdb['cluster_size'].values)\n",
    "stdb_SAIFI_denom = len(pw['core_id'].unique())*(len(stdb))\n",
    "stdb_SAIFI = stdb_SAIFI_num/stdb_SAIFI_denom\n",
    "stdb_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}