{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for IPSN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the notebook: \n",
    "\n",
    "1) Clustering and analysis for DB, Agglom, STDB which are ultimately stored in the dataframes: *db, agglom, stdb*\n",
    "\n",
    "2) Plots that Josh and Noah requested\n",
    "\n",
    "\n",
    "Disclaimers:\n",
    "- I plotted each clustering method separately instead of all compiled on one graph \n",
    "- Agglom logical distance hasn't been computed because the agglomerative csv I pulled doesn't have core_id's and I didn't want to mess with it \n",
    "- be wary about the order in which you run cells \n",
    "    - (the code is super repetitive since I pulled from a bunch of different notebooks, but I did my best to make it user friendly)\n",
    "    \n",
    "Let me know if you run into any problems! - Emily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install all of these libraries before beginning \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time \n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics \n",
    "import re\n",
    "import pylab\n",
    "from scipy.stats import norm\n",
    "import geopandas as gpd \n",
    "import shapely.geometry as geometry\n",
    "import shapely.ops as ops \n",
    "from functools import partial \n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transition points of the outages from July 2018 pulled from outage_aggregator.py\n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all of the data from July 2018 (not just outage transition points!) \n",
    "#only use this for SAIFI and convex hull calculations \n",
    "pw = pd.read_parquet('part-00000-602cb425-c6be-40be-8024-aeb92fcb4315-c000.gz.parquet').drop(['product_id', 'millis', 'last_plug_millis', 'last_unplug_millis'], axis=1)\n",
    "pw_powered = pw[~pw['is_powered']]\n",
    "pw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical grid distance csv \n",
    "logical = pd.read_csv('/Users/emilypaszkiewicz17/gridwatch-data-analysis/grid_distance.csv')\n",
    "logical = logical.reset_index()\n",
    "logical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN(time only): Clustering and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN(time) on a month's worth of data \n",
    "month_out = outages\n",
    "month_out['z'] = 0\n",
    "X=month_out[['outage_time', 'z']]\n",
    "out_cluster = StandardScaler().fit_transform(X)\n",
    "db = DBSCAN(eps=0.0001, algorithm='ball_tree', min_samples=2).fit(out_cluster)\n",
    "labels = db.labels_\n",
    "no_noise = list(labels).count(-1)\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "no_noise, no_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_out['labels'] =labels \n",
    "month_out['core_id'] = outages['core_id']\n",
    "month_out.head()\n",
    "month_clust = pd.DataFrame(month_out.groupby(['labels'])['outage_time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = month_out.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = month_out.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['ids'] = month_out.groupby(['labels'])['core_id'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "\n",
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "month_clust.head()\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['outage_time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "month_clust['cluster_size'] = month_out.groupby(['labels'])['outage_time'].count()\n",
    "month_clust['min_time'] = month_clust['outage_time'].apply(lambda x: min(x))\n",
    "month_clust['max_time'] = month_clust['outage_time'].apply(lambda x: max(x))\n",
    "\n",
    "\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI this cell will take a few mins to run on a month's worth of data\n",
    "\n",
    "#check if powered points are within the time range of the clusters \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage\n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = []\n",
    "within_time_long = []\n",
    "for o_index in range(len(month_clust)): \n",
    "    nest_lat = []\n",
    "    nest_long= []\n",
    "    for p_index in range(len(powered)): \n",
    "        if powered['time'].values[p_index] >= month_clust['min_time'].values[o_index] and powered['time'].values[p_index] <= month_clust['max_time'].values[o_index]:\n",
    "            nest_lat.append(powered['location_latitude'].values[p_index])\n",
    "            nest_long.append(powered['location_longitude'].values[p_index])        \n",
    "    within_time_lat.append(nest_lat)\n",
    "    within_time_long.append(nest_long)\n",
    "            \n",
    "month_clust['within_time_lat'] = within_time_lat\n",
    "month_clust['within_time_long'] = within_time_long\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = month_clust['latitude'].copy()\n",
    "update_long = month_clust['longitude'].copy()\n",
    "update_within_lat = month_clust['within_time_lat'].copy()\n",
    "update_within_long = month_clust['within_time_long'].copy()\n",
    "\n",
    "for i in range(len(month_clust)): \n",
    "    if len(month_clust.iloc[i]['within_time_lat']) == 0: \n",
    "        update_within_lat.values[i] = [0, 1, 2]\n",
    "        update_within_long.values[i] = [0, 1, 2] \n",
    "    if len(update_within_lat.values[i]) < 3: \n",
    "        update_within_lat.values[i] = month_clust.iloc[i]['within_time_lat']*3\n",
    "        update_within_long.values[i] = month_clust.iloc[i]['within_time_long']*3        \n",
    "    if month_clust.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = month_clust.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = month_clust.iloc[i]['longitude']*2\n",
    "\n",
    "month_clust['latitude'] = update_lat\n",
    "month_clust['longitude'] = update_long\n",
    "month_clust['within_time_long'] = update_within_long\n",
    "month_clust['within_time_lat'] = update_within_lat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create geodataframes to calculate convex hull \n",
    "power = month_clust.copy()\n",
    "out = month_clust.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(month_clust)):\n",
    "    a = month_clust.iloc[i, :]['within_time_long']\n",
    "    b = month_clust.iloc[i, :]['within_time_lat']\n",
    "    c = month_clust.iloc[i, :]['longitude']\n",
    "    d = month_clust.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "month_clust['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "month_clust['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "month_clust['powered_poly'] = (np.vectorize(unique_coords)(month_clust['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "month_clust['outage_poly'] = (np.vectorize(unique_coords)(month_clust['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "\n",
    "\n",
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "def outage_size(outage_coords): \n",
    "    return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "db = out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative: Cleaning and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading outages from the pw_finalized_with_string dataframe from outage_aggregator.py and doing some data cleaning \n",
    "spark_outages = pd.read_parquet('part-00000-1a77f616-ace0-482c-9ad1-bdc53a8286bc-c000.gz.parquet')\n",
    "\n",
    "spark_outages = spark_outages[spark_outages['cluster_size'] > 1]\n",
    "spark_day = spark_outages\n",
    "spark_day['outage_times'] = spark_day['outage_times'].apply(lambda x: re.findall('\\d+', x))\n",
    "spark_day['location'] = spark_day['location'].apply(lambda x: re.findall('\\d.\\d+', x))\n",
    "spark_day_exploded = spark_day.explode('outage_times')\n",
    "spark_day_exploded['outage_times'] = spark_day_exploded['outage_times'].apply(lambda x: int(x))\n",
    "unexploded = spark_day_exploded.groupby('outage_time')['outage_times'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')\n",
    "unexploded['location'] = spark_day.sort_values('outage_time')['location'].values\n",
    "\n",
    "explode_loc = unexploded.explode('location')\n",
    "explode_loc['location'] = explode_loc['location'].apply(lambda x: float(x))\n",
    "lat = explode_loc[explode_loc['location'] > 1]\n",
    "long = explode_loc[explode_loc['location'] < 1]\n",
    "lat['latitude'] = lat['location']\n",
    "long['longitude'] = long['location']*(-1)\n",
    "lat = lat.groupby('outage_time')['latitude'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')\n",
    "lat['longitude'] = long.groupby('outage_time')['longitude'].apply(lambda x: x.to_list()).reset_index().sort_values('outage_time')['longitude']\n",
    "lat['cluster_size'] = spark_day.sort_values('outage_time')['cluster_size'].values\n",
    "lat['outage_times_stddev'] = spark_day.sort_values('outage_time')['outage_times_stddev'].values\n",
    "lat['range'] = spark_day.sort_values('outage_time')['outage_times_range'].values\n",
    "lat['outage_times'] = unexploded.sort_values('outage_time')['outage_times']\n",
    "spark_day = lat\n",
    "spark_day['min_time'] = spark_day['outage_times'].apply(lambda x: min(x))\n",
    "spark_day['max_time'] = spark_day['outage_times'].apply(lambda x: max(x))\n",
    "\n",
    "spark_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI this cell will take a few mins to run on a month's worth of data\n",
    "\n",
    "#Now let's compute the number of sensors within the convex hull of the outage \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage \n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = []\n",
    "within_time_long = []\n",
    "\n",
    "for o_index in range(len(spark_day)): \n",
    "    nest_lat = []\n",
    "    nest_long= []\n",
    "    for p_index in range(len(powered)): \n",
    "        if powered['time'].values[p_index] >= spark_day['min_time'].values[o_index] and powered['time'].values[p_index] <= spark_day['max_time'].values[o_index]:\n",
    "            nest_lat.append(powered['location_latitude'].values[p_index])\n",
    "            nest_long.append(powered['location_longitude'].values[p_index])        \n",
    "    within_time_lat.append(nest_lat)\n",
    "    within_time_long.append(nest_long)\n",
    "            \n",
    "spark_day['within_time_lat'] = within_time_lat\n",
    "spark_day['within_time_long'] = within_time_long\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = spark_day['latitude'].copy()\n",
    "update_long = spark_day['longitude'].copy()\n",
    "update_within_lat = spark_day['within_time_lat'].copy()\n",
    "update_within_long = spark_day['within_time_long'].copy()\n",
    "\n",
    "for i in range(len(spark_day)): \n",
    "    if len(spark_day.iloc[i]['within_time_lat']) == 0: \n",
    "        update_within_lat.values[i] = [0, 1, 2]\n",
    "        update_within_long.values[i] = [0, 1, 2] \n",
    "    if len(update_within_lat.values[i]) < 3: \n",
    "        update_within_lat.values[i] = spark_day.iloc[i]['within_time_lat']*3\n",
    "        update_within_long.values[i] = spark_day.iloc[i]['within_time_long']*3        \n",
    "    if spark_day.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = spark_day.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = spark_day.iloc[i]['longitude']*2\n",
    "\n",
    "spark_day['latitude'] = update_lat\n",
    "spark_day['longitude'] = update_long\n",
    "spark_day['within_time_long'] = update_within_long\n",
    "spark_day['within_time_lat'] = update_within_lat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create geodataframes to calculate convex hull \n",
    "power = spark_day.copy()\n",
    "out = spark_day.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(spark_day)):\n",
    "    a = spark_day.iloc[i, :]['within_time_long']\n",
    "    b = spark_day.iloc[i, :]['within_time_lat']\n",
    "    c = spark_day.iloc[i, :]['longitude']\n",
    "    d = spark_day.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "spark_day['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "spark_day['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "spark_day['powered_poly'] = (np.vectorize(unique_coords)(spark_day['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "spark_day['outage_poly'] = (np.vectorize(unique_coords)(spark_day['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "\n",
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "def outage_size(outage_coords): \n",
    "    return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "\n",
    "agglom = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STDBSCAN: Clustering and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to initiate the STDBSCAN class \n",
    "\n",
    "class STDBSCAN(object):\n",
    "\n",
    "    def __init__(self, col_lat, col_lon, col_time, spatial_threshold=500.0, \n",
    "                 temporal_threshold=60.0, min_neighbors=15):\n",
    "        \"\"\"\n",
    "        Python st-dbscan implementation.\n",
    "        :param col_lat: Latitude column name;\n",
    "        :param col_lon:  Longitude column name;\n",
    "        :param col_time: Date time column name;\n",
    "        :param spatial_threshold: Maximum geographical coordinate (spatial)\n",
    "             distance value (meters);\n",
    "        :param temporal_threshold: Maximum non-spatial distance value (seconds);\n",
    "        :param min_neighbors: Minimum number of points within Eps1 and Eps2\n",
    "             distance;\n",
    "        \"\"\"\n",
    "        self.col_lat = col_lat\n",
    "        self.col_lon = col_lon\n",
    "        self.col_time = col_time\n",
    "        self.spatial_threshold = spatial_threshold\n",
    "        self.temporal_threshold = temporal_threshold\n",
    "        self.min_neighbors = min_neighbors\n",
    "\n",
    "    def projection(self, df, p1_str='epsg:4326', p2_str='epsg:3395'):\n",
    "        \"\"\"\n",
    "        Cython wrapper to converts from geographic (longitude,latitude)\n",
    "        to native map projection (x,y) coordinates. It needs to select the\n",
    "        right epsg. Values of x and y are given in meters\n",
    "        \"\"\"\n",
    "        p1 = pyproj.Proj(init=p1_str)\n",
    "        p2 = pyproj.Proj(init=p2_str)\n",
    "        lon = df[self.col_lon].values\n",
    "        lat = df[self.col_lat].values\n",
    "        x1, y1 = p1(lon, lat)\n",
    "        x2, y2 = pyproj.transform(p1, p2, x1, y1, radians=True)\n",
    "        df[self.col_lon] = x2\n",
    "        df[self.col_lat] = y2\n",
    "\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def _retrieve_neighbors(self, index_center, matrix):\n",
    "\n",
    "        center_point = matrix[index_center, :]\n",
    "\n",
    "        # filter by time\n",
    "        min_time = center_point[2] - timedelta(seconds=self.temporal_threshold)\n",
    "        max_time = center_point[2] + timedelta(seconds=self.temporal_threshold)\n",
    "        matrix = matrix[(matrix[:, 2] >= min_time) &\n",
    "                        (matrix[:, 2] <= max_time), :]\n",
    "        # filter by distance\n",
    "        tmp = (matrix[:, 0]-center_point[0])*(matrix[:, 0]-center_point[0]) + \\\n",
    "            (matrix[:, 1]-center_point[1])*(matrix[:, 1]-center_point[1])\n",
    "        neigborhood = matrix[tmp <= (\n",
    "            self.spatial_threshold*self.spatial_threshold), 4].tolist()\n",
    "        neigborhood.remove(index_center)\n",
    "\n",
    "        return neigborhood\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            df={o1,o2,...,on} Set of objects;\n",
    "        OUTPUT:\n",
    "            C = {c1,c2,...,ck} Set of clusters\n",
    "        \"\"\"\n",
    "        cluster_label = 0\n",
    "        noise = -1\n",
    "        unmarked = 777777\n",
    "        stack = []\n",
    "\n",
    "        # initial setup\n",
    "        df = df[[self.col_lon, self.col_lat, self.col_time]]\n",
    "        df = df.assign(cluster=unmarked)\n",
    "        df['index'] = range(df.shape[0])\n",
    "        matrix = df.values\n",
    "        df.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "        # for each point in database\n",
    "        for index in range(matrix.shape[0]):\n",
    "            if matrix[index, 3] == unmarked:\n",
    "                neighborhood = self._retrieve_neighbors(index, matrix)\n",
    "\n",
    "                if len(neighborhood) < self.min_neighbors:\n",
    "                    matrix[index, 3] = noise\n",
    "                else:  # found a core point\n",
    "                    cluster_label += 1\n",
    "                    # assign a label to core point\n",
    "                    matrix[index, 3] = cluster_label\n",
    "\n",
    "                    # assign core's label to its neighborhood\n",
    "                    for neig_index in neighborhood:\n",
    "                        matrix[neig_index, 3] = cluster_label\n",
    "                        stack.append(neig_index)  # append neighbors to stack\n",
    "\n",
    "                    # find new neighbors from core point neighborhood\n",
    "                    while len(stack) > 0:\n",
    "                        current_point_index = stack.pop()\n",
    "                        new_neighborhood = \\\n",
    "                            self._retrieve_neighbors(current_point_index,\n",
    "                                                     matrix)\n",
    "\n",
    "                        # current_point is a new core\n",
    "                        if len(new_neighborhood) >= self.min_neighbors:\n",
    "                            for neig_index in new_neighborhood:\n",
    "                                neig_cluster = matrix[neig_index, 3]\n",
    "                                if any([neig_cluster == noise,\n",
    "                                        neig_cluster == unmarked]):\n",
    "                                    matrix[neig_index, 3] = cluster_label\n",
    "                                    stack.append(neig_index)\n",
    "\n",
    "        df['cluster'] = matrix[:, 3]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where you actually adjust the parameters \n",
    "\n",
    "def test_time(df):\n",
    "    '''\n",
    "    transfrom the lon and lat to x and y\n",
    "    need to select the right epsg\n",
    "    I don't the true epsg of sample, but get the same result by using \n",
    "    epsg:4326 and epsg:32635\n",
    "    '''\n",
    "    st_dbscan = STDBSCAN(col_lat='location_latitude', col_lon='location_longitude',\n",
    "                         col_time='time', spatial_threshold=0.03,\n",
    "                         temporal_threshold=60, min_neighbors=1)\n",
    "    #df = st_dbscan.projection(df, p1_str='epsg:4326', p2_str='epsg:32630')\n",
    "    return st_dbscan.run(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transition points of the outages \n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages['time'] = outages['outage_time'].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "outages['time'] = pd.to_datetime(outages['time'], infer_datetime_format=True)\n",
    "\n",
    "stdb_clustered = pd.DataFrame(test_time(outages))\n",
    "stdb_clustered['core_id'] = outages['core_id']\n",
    "\n",
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "stdb_clustered['time'] = stdb_clustered['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "month_clust = pd.DataFrame(stdb_clustered.groupby(['cluster'])['time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = stdb_clustered.groupby(['cluster'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = stdb_clustered.groupby(['cluster'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['ids'] = stdb_clustered.groupby(['cluster'])['core_id'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "month_clust['cluster_size'] = stdb_clustered.groupby(['cluster'])['time'].count()\n",
    "month_clust['min_time'] = month_clust['time'].apply(lambda x: min(x))\n",
    "month_clust['max_time'] = month_clust['time'].apply(lambda x: max(x))\n",
    "\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI this cell will take a few mins to run on a month's worth of data\n",
    "\n",
    "#check if powered points are within the time range of the clusters \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage\n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = []\n",
    "within_time_long = []\n",
    "for o_index in range(len(month_clust)): \n",
    "    nest_lat = []\n",
    "    nest_long= []\n",
    "    for p_index in range(len(powered)): \n",
    "        if powered['time'].values[p_index] >= month_clust['min_time'].values[o_index] and powered['time'].values[p_index] <= month_clust['max_time'].values[o_index]:\n",
    "            nest_lat.append(powered['location_latitude'].values[p_index])\n",
    "            nest_long.append(powered['location_longitude'].values[p_index])        \n",
    "    within_time_lat.append(nest_lat)\n",
    "    within_time_long.append(nest_long)\n",
    "            \n",
    "month_clust['within_time_lat'] = within_time_lat\n",
    "month_clust['within_time_long'] = within_time_long\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = month_clust['latitude'].copy()\n",
    "update_long = month_clust['longitude'].copy()\n",
    "update_within_lat = month_clust['within_time_lat'].copy()\n",
    "update_within_long = month_clust['within_time_long'].copy()\n",
    "\n",
    "for i in range(len(month_clust)): \n",
    "    if len(month_clust.iloc[i]['within_time_lat']) == 0: \n",
    "        update_within_lat.values[i] = [0, 1, 2]\n",
    "        update_within_long.values[i] = [0, 1, 2] \n",
    "    if len(update_within_lat.values[i]) < 3: \n",
    "        update_within_lat.values[i] = month_clust.iloc[i]['within_time_lat']*3\n",
    "        update_within_long.values[i] = month_clust.iloc[i]['within_time_long']*3        \n",
    "    if month_clust.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = month_clust.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = month_clust.iloc[i]['longitude']*2\n",
    "\n",
    "month_clust['latitude'] = update_lat\n",
    "month_clust['longitude'] = update_long\n",
    "month_clust['within_time_long'] = update_within_long\n",
    "month_clust['within_time_lat'] = update_within_lat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create geodataframes to calculate convex hull \n",
    "power = month_clust.copy()\n",
    "out = month_clust.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(month_clust)):\n",
    "    a = month_clust.iloc[i, :]['within_time_long']\n",
    "    b = month_clust.iloc[i, :]['within_time_lat']\n",
    "    c = month_clust.iloc[i, :]['longitude']\n",
    "    d = month_clust.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "month_clust['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "month_clust['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "month_clust['powered_poly'] = (np.vectorize(unique_coords)(month_clust['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "month_clust['outage_poly'] = (np.vectorize(unique_coords)(month_clust['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "\n",
    "\n",
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "def outage_size(outage_coords): \n",
    "    return len(pd.Series(outage_coords).unique())\n",
    "\n",
    "out['powered_size_within_outage_area'] = (np.vectorize(outage_size)(out['powered_within_outage']))\n",
    "out['percent_pow_within_outage'] = (out['powered_size_within_outage_area'] / (out['powered_size_within_outage_area'] + out['cluster_size']))*100\n",
    "\n",
    "\n",
    "stdb = out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 1: Trimodal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "db_clust_sizes = pd.DataFrame(db.groupby('cluster_size')['ids'].nunique()).reset_index()\n",
    "sns.barplot(x='cluster_size', y='ids', data=db_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('DBSCAN: Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGLOM\n",
    "agglom_clust_sizes = pd.DataFrame(agglom.groupby('cluster_size')['latitude'].nunique()).reset_index()\n",
    "sns.barplot(x='cluster_size', y='latitude', data=agglom_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('Agglomerative: Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN \n",
    "stdb_clust_sizes = pd.DataFrame(stdb.groupby('cluster_size')['ids'].nunique()).reset_index()\n",
    "sns.barplot(x='cluster_size', y='ids', data=stdb_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('STDBSCAN: Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2a: Low Voltage Success (Euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Euclidean \n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "c = month_out.groupby('labels').count()\n",
    "pair_index = c[c['time'] == 2].index\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for clusters of 2: \n",
    "pairs = month_out[month_out['labels'].isin(pair_index)]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.location_longitude, pairs.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('labels')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('labels')['geometry'].last(), crs={'init':'aea'})\n",
    "db_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = month_out[month_out['labels'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "db_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "db_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "db_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#percentage under 550m for pairs: \n",
    "db_pair_percent_under_550= len(db_distances[db_distances < 550])/len(db_distances)\n",
    "\n",
    "#percentage under 550m for trios: \n",
    "db_dist_for_3 = list(db_trios_distances['1->2'].values) + list(db_trios_distances['2->3'].values) + list(db_trios_distances['3->1'].values)\n",
    "db_dist_for_3 = np.array(db_dist_for_3)\n",
    "db_trio_percent_under_550 = len(db_dist_for_3[db_dist_for_3 < 550])/len(db_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_550, db_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGLOMERATIVE EUCLIDEAN\n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "t = spark_outages[spark_outages['cluster_size'] <= 3]\n",
    "explode_loc = t.explode('location')\n",
    "explode_loc['location'] = explode_loc['location'].apply(lambda x: float(x))\n",
    "lat = explode_loc[explode_loc['location'] > 1]\n",
    "long = explode_loc[explode_loc['location'] < 1]\n",
    "lat['latitude'] = lat['location']\n",
    "t = lat[['outage_time', 'outage_times', 'cluster_size', 'latitude']]\n",
    "t['longitude'] = long['location']*(-1)\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for sensors of cluster size 2 \n",
    "pairs = t[t['cluster_size'] == 2]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.longitude, pairs.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "agglom_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#dustances for sensors of cluster size 3 \n",
    "trios = t[t['cluster_size'] == 3]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.longitude, trios.latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('outage_time')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "agglom_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "agglom_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "agglom_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "agglom_trios_distances\n",
    "\n",
    "#I would use agglom_trios_distances_list for measuring clustering success \n",
    "agglom_dist_for_3 = list(agglom_trios_distances['1->2'].values) + list(agglom_trios_distances['2->3'].values) + list(agglom_trios_distances['3->1'].values)\n",
    "\n",
    "#now let's caclulate the percentage within 550 m \n",
    "#percentage under the cutoff for pairs: \n",
    "agglom_pair_percent_under_550 = len(agglom_distances[agglom_distances < 550])/len(agglom_distances)\n",
    "\n",
    "#percentage under the cutoff for trios: \n",
    "agglom_dist_for_3 = np.array(agglom_dist_for_3)\n",
    "agglom_trio_percent_under_550 = len(agglom_dist_for_3[agglom_dist_for_3 < 550])/len(agglom_dist_for_3)\n",
    "\n",
    "agglom_pair_percent_under_550, agglom_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN Euclidean \n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "c = stdb_clustered.groupby('cluster').count()\n",
    "pair_index = c[c['time'] == 2].index\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for clusters of 2: \n",
    "pairs = stdb_clustered[stdb_clustered['cluster'].isin(pair_index)]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.location_longitude, pairs.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "stdb_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = stdb_clustered[stdb_clustered['cluster'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('cluster')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "stdb_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "stdb_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "stdb_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#percentage under 550m for pairs: \n",
    "stdb_pair_percent_under_550= len(stdb_distances[stdb_distances < 550])/len(stdb_distances)\n",
    "\n",
    "#percentage under 550m for trios: \n",
    "stdb_dist_for_3 = list(stdb_trios_distances['1->2'].values) + list(stdb_trios_distances['2->3'].values) + list(stdb_trios_distances['3->1'].values)\n",
    "stdb_dist_for_3 = np.array(stdb_dist_for_3)\n",
    "stdb_trio_percent_under_550 = len(stdb_dist_for_3[stdb_dist_for_3 < 550])/len(stdb_dist_for_3)\n",
    "\n",
    "stdb_pair_percent_under_550, stdb_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2b: Low Voltage Success (Logical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Logical Grid \n",
    "\n",
    "two_ids = db[db['cluster_size'] == 2]\n",
    "three_ids =  db[db['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3\n",
    "db_logical_pairs = two_ids \n",
    "db_logical_trios = three_ids\n",
    "two_ids\n",
    "\n",
    "\n",
    "#calculate the % of outage pairs that are under the same transformer \n",
    "db_pair_percent_under_same_transformer = len(db_logical_pairs[db_logical_pairs['logical_distance'] ==1])/len(db_logical_pairs)\n",
    "\n",
    "#calculate the % of outage trios that are under the same transformer \n",
    "log_dist_for_3 = list(db_logical_trios['log_dist_1']) + list(db_logical_trios['log_dist_2']) + list(db_logical_trios['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "db_trio_percent_under_same_transformer = len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_same_transformer, db_trio_percent_under_same_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglom Logical Grid \n",
    "\n",
    "#we have to wait for agglom to have core_id's in order to calculate logical grid dist \n",
    "#this needs to happen either by merging dataframes or by adding them in spark\n",
    "#once you have the core_id's you should be able to easily copy and paste the code for the DBSCANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN Logical Grid \n",
    "\n",
    "two_ids = stdb[stdb['cluster_size'] == 2]\n",
    "three_ids =  stdb[stdb['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3\n",
    "stdb_logical_pairs = two_ids \n",
    "stdb_logical_trios = three_ids\n",
    "\n",
    "\n",
    "#calculate the % of outage pairs that are under the same transformer \n",
    "stdb_pair_percent_under_same_transformer = len(stdb_logical_pairs[stdb_logical_pairs['logical_distance'] ==1])/len(stdb_logical_pairs)\n",
    "\n",
    "#calculate the % of outage trios that are under the same transformer \n",
    "log_dist_for_3 = list(stdb_logical_trios['log_dist_1']) + list(stdb_logical_trios['log_dist_2']) + list(stdb_logical_trios['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "stdb_trio_percent_under_same_transformer = len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)\n",
    "\n",
    "stdb_pair_percent_under_same_transformer, stdb_trio_percent_under_same_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3: Outage Size v. time variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_size_time = pd.DataFrame(db.groupby('cluster_size')['time_range'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='time_range', data=db_size_time)\n",
    "plt.title('DBSCAN: Outage Size v. Average Time Range of Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agglom_size_time = pd.DataFrame(agglom.groupby('cluster_size')['range'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='range', data=agglom_size_time)\n",
    "plt.title('Agglomerative: Outage Size v. Average Time Range of Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdb_size_time = pd.DataFrame(stdb.groupby('cluster_size')['time_range'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='time_range', data=stdb_size_time)\n",
    "plt.title('STDBSCAN: Outage Size v. Average Time Range of Outage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 4: Percent in Covex Hull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_convex_hull = pd.DataFrame(db.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=db_convex_hull)\n",
    "plt.title('DBSCAN: Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglom_convex_hull = pd.DataFrame(agglom.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=agglom_convex_hull)\n",
    "plt.title('Agglomerative: Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdb_convex_hull = pd.DataFrame(stdb.groupby('cluster_size')['percent_pow_within_outage'].apply(np.mean)).reset_index()\n",
    "sns.lineplot(x='cluster_size', y='percent_pow_within_outage', data=stdb_convex_hull)\n",
    "plt.title('STDBSCAN: Outage Size v. Average Percent Sensors within the Convex Hull of the Outage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 5: SAFI Calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAIFI Calculations are currently being calculated for the entire time period. Make sure to split the data into July, Aug, Sept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN \n",
    "db_SAIFI_num = sum(db['cluster_size'].values)\n",
    "db_SAIFI_denom = len(pw['core_id'].unique())*(len(db))\n",
    "db_SAIFI = db_SAIFI_num/db_SAIFI_denom\n",
    "db_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative \n",
    "agglom_SAIFI_num = sum(agglom['cluster_size'].values)\n",
    "agglom_SAIFI_denom = len(pw['core_id'].unique())*(len(agglom))\n",
    "agglom_SAIFI = agglom_SAIFI_num/agglom_SAIFI_denom\n",
    "agglom_SAIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STDBSCAN\n",
    "stdb_SAIFI_num = sum(stdb['cluster_size'].values)\n",
    "stdb_SAIFI_denom = len(pw['core_id'].unique())*(len(stdb))\n",
    "stdb_SAIFI = stdb_SAIFI_num/stdb_SAIFI_denom\n",
    "stdb_SAIFI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}