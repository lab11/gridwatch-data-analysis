{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time \n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics \n",
    "import re\n",
    "import pylab\n",
    "from scipy.stats import norm\n",
    "import geopandas as gpd \n",
    "import shapely.geometry as geometry\n",
    "import shapely.ops as ops \n",
    "from functools import partial \n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transition points of the outages from July 2018\n",
    "outages = pd.read_parquet('part-00000-3c7aa0ea-41c7-4705-bafc-5662f2051563-c000.gz.parquet')\n",
    "outages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all of the data from July 2018 (not just outage transition points!) \n",
    "#only use this for SAIFI and convex hull calculations \n",
    "pw = pd.read_parquet('part-00000-602cb425-c6be-40be-8024-aeb92fcb4315-c000.gz.parquet').drop(['product_id', 'millis', 'last_plug_millis', 'last_unplug_millis'], axis=1)\n",
    "pw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical grid distance csv \n",
    "logical = pd.read_csv('/Users/emilypaszkiewicz17/gridwatch-data-analysis/grid_distance.csv')\n",
    "logical = logical.reset_index()\n",
    "logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN(time) on a month's worth of data \n",
    "month_out = outages\n",
    "month_out['z'] = 0\n",
    "X=month_out[['outage_time', 'z']]\n",
    "out_cluster = StandardScaler().fit_transform(X)\n",
    "db = DBSCAN(eps=0.0001, algorithm='ball_tree', min_samples=2).fit(out_cluster)\n",
    "labels = db.labels_\n",
    "no_noise = list(labels).count(-1)\n",
    "no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "no_noise, no_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_out['labels'] =labels \n",
    "month_out['core_id'] = outages['core_id']\n",
    "month_out.head()\n",
    "month_clust = pd.DataFrame(month_out.groupby(['labels'])['outage_time'].apply(lambda x: x.tolist()))\n",
    "month_clust['latitude'] = month_out.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['longitude'] = month_out.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "month_clust['ids'] = month_out.groupby(['labels'])['core_id'].apply(lambda x: x.tolist()).values\n",
    "month_clust = month_clust.iloc[1:]\n",
    "\n",
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "month_clust.head()\n",
    "month_clust['time_range'] = (np.vectorize(find_range)(month_clust['outage_time']))\n",
    "month_clust['lat_range'] = (np.vectorize(find_range)(month_clust['latitude']))\n",
    "month_clust['long_range'] = (np.vectorize(find_range)(month_clust['longitude']))\n",
    "\n",
    "month_clust['cluster_size'] = month_out.groupby(['labels'])['outage_time'].count()\n",
    "month_clust['min_time'] = month_clust['outage_time'].apply(lambda x: min(x))\n",
    "month_clust['max_time'] = month_clust['outage_time'].apply(lambda x: max(x))\n",
    "\n",
    "\n",
    "month_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI this cell will take a few mins to run on a month's worth of data\n",
    "\n",
    "#check if powered points are within the time range of the clusters \n",
    "#we will start by computing the lat and long values for powered sensors that reported within the time range of the outage\n",
    "#then, later we will compare these powered coords to see if they are also within the convex hull of the outage\n",
    "powered = pw[~pw['is_powered']]\n",
    "powered['time'] = powered['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "within_time_lat = []\n",
    "within_time_long = []\n",
    "for o_index in range(len(month_clust)): \n",
    "    nest_lat = []\n",
    "    nest_long= []\n",
    "    for p_index in range(len(powered)): \n",
    "        if powered['time'].values[p_index] >= month_clust['min_time'].values[o_index] and powered['time'].values[p_index] <= month_clust['max_time'].values[o_index]:\n",
    "            nest_lat.append(powered['location_latitude'].values[p_index])\n",
    "            nest_long.append(powered['location_longitude'].values[p_index])        \n",
    "    within_time_lat.append(nest_lat)\n",
    "    within_time_long.append(nest_long)\n",
    "            \n",
    "month_clust['within_time_lat'] = within_time_lat\n",
    "month_clust['within_time_long'] = within_time_long\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for clusters of 2, duplicate the lat and long points so that these points can also be converted into a Polygon by geopandas \n",
    "update_lat = month_clust['latitude'].copy()\n",
    "update_long = month_clust['longitude'].copy()\n",
    "update_within_lat = month_clust['within_time_lat'].copy()\n",
    "update_within_long = month_clust['within_time_long'].copy()\n",
    "\n",
    "for i in range(len(month_clust)): \n",
    "    if len(month_clust.iloc[i]['within_time_lat']) == 0: \n",
    "        update_within_lat.values[i] = [0, 1, 2]\n",
    "        update_within_long.values[i] = [0, 1, 2] \n",
    "    if len(update_within_lat.values[i]) < 3: \n",
    "        update_within_lat.values[i] = month_clust.iloc[i]['within_time_lat']*3\n",
    "        update_within_long.values[i] = month_clust.iloc[i]['within_time_long']*3        \n",
    "    if month_clust.iloc[i]['cluster_size'] < 3: \n",
    "        update_lat.values[i] = month_clust.iloc[i]['latitude']*2\n",
    "        update_long.values[i] = month_clust.iloc[i]['longitude']*2\n",
    "\n",
    "month_clust['latitude'] = update_lat\n",
    "month_clust['longitude'] = update_long\n",
    "month_clust['within_time_long'] = update_within_long\n",
    "month_clust['within_time_lat'] = update_within_lat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create geodataframes to calculate convex hull \n",
    "power = month_clust.copy()\n",
    "out = month_clust.copy()\n",
    "powered_poly = []\n",
    "outage_poly = []\n",
    "for i in range(len(month_clust)):\n",
    "    a = month_clust.iloc[i, :]['within_time_long']\n",
    "    b = month_clust.iloc[i, :]['within_time_lat']\n",
    "    c = month_clust.iloc[i, :]['longitude']\n",
    "    d = month_clust.iloc[i, :]['latitude']\n",
    "    powered_poly.append(list(zip(a, b)))\n",
    "    outage_poly.append(list(zip(c, d)))\n",
    "    \n",
    "def unique_coords(coords):\n",
    "    return pd.Series(coords).unique()\n",
    "\n",
    "power['powered_poly'] = powered_poly\n",
    "out['powered_poly'] = powered_poly\n",
    "month_clust['powered_poly'] = powered_poly\n",
    "out['outage_poly'] = outage_poly\n",
    "power['outage_poly'] = outage_poly\n",
    "month_clust['outage_poly'] = outage_poly\n",
    "crs = {'init', 'epsg:4326'}\n",
    "\n",
    "powered_poly = [geometry.Polygon(x, holes=None) for x in power['powered_poly']]\n",
    "power = gpd.GeoDataFrame(power, crs=crs, geometry=(powered_poly))\n",
    "\n",
    "outage_poly = [geometry.Polygon(x, holes=None) for x in out['outage_poly']]\n",
    "out= gpd.GeoDataFrame(out, crs=crs, geometry=(outage_poly))\n",
    "\n",
    "\n",
    "power['powered_poly'] = (np.vectorize(unique_coords)(power['powered_poly']))\n",
    "out['powered_poly'] = (np.vectorize(unique_coords)(out['powered_poly']))\n",
    "month_clust['powered_poly'] = (np.vectorize(unique_coords)(month_clust['powered_poly']))\n",
    "out['outage_poly'] = (np.vectorize(unique_coords)(out['outage_poly']))\n",
    "power['outage_poly'] = (np.vectorize(unique_coords)(power['outage_poly']))\n",
    "month_clust['outage_poly'] = (np.vectorize(unique_coords)(month_clust['outage_poly']))\n",
    "\n",
    "power['convex_area_powered'] = power.convex_hull\n",
    "out['convex_area_outage'] = out.convex_hull\n",
    "\n",
    "\n",
    "\n",
    "#calculate the convex hull \n",
    "def in_convex_hull(powered_coords, geom):\n",
    "#takes in lat/long pairs in powered_coords, and a Polygon to chekc if the point is within the convex hull of the Polygon \n",
    "    in_convex_hull = []\n",
    "    for i in powered_coords: \n",
    "        if geom.convex_hull.contains(geometry.Point(i)):\n",
    "            in_convex_hull.append(i)\n",
    "    in_convex_hull = pd.Series(in_convex_hull).unique() \n",
    "    return in_convex_hull\n",
    "        \n",
    "in_convex_hull = [in_convex_hull(out['powered_poly'].values[i], out['geometry'].values[i]) for i in range(len(out))]\n",
    "out['powered_within_outage'] = in_convex_hull\n",
    "\n",
    "db = out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "db_clust_sizes = pd.DataFrame(db.groupby('cluster_size')['ids'].nunique()).reset_index()\n",
    "sns.barplot(x='cluster_size', y='ids', data=db_clust_sizes)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.title('Cluster Size vs. Number of Clusters of this Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Euclidean \n",
    "\n",
    "#now let's make some new dataframes so that we can calculate the distances between the sensors of 2-3 that were clustered together \n",
    "c = month_out.groupby('labels').count()\n",
    "pair_index = c[c['time'] == 2].index\n",
    "\n",
    "def calc_dist(dist1, dist2): \n",
    "#this function takes two geometric points in and computes the distance in meters between them \n",
    "    one = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist1)\n",
    "    two = ops.transform(partial(\n",
    "        pyproj.transform, \n",
    "        pyproj.Proj(init='EPSG:4326'), \n",
    "        pyproj.Proj(proj='aea')), dist2)\n",
    "    return one.distance(two)\n",
    "\n",
    "#distances for clusters of 2: \n",
    "pairs = month_out[month_out['labels'].isin(pair_index)]\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pairs, geometry=gpd.points_from_xy(pairs.location_longitude, pairs.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1 = gpd.GeoSeries(gdf.groupby('labels')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2 = gpd.GeoSeries(gdf.groupby('labels')['geometry'].last(), crs={'init':'aea'})\n",
    "db_distances = (np.vectorize(calc_dist)(dist_1, dist_2))\n",
    "\n",
    "#distances for clusters of 3: \n",
    "trio_index = c[c['time'] == 3].index\n",
    "trios = month_out[month_out['labels'].isin(trio_index)]\n",
    "gdf_trios = gpd.GeoDataFrame(\n",
    "    trios, geometry=gpd.points_from_xy(trios.location_longitude, trios.location_latitude), crs={'init':'epsg:4326'})\n",
    "dist_1_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].first(), crs={'init':'aea'})\n",
    "dist_2_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].nth(1), crs={'init':'aea'})\n",
    "dist_3_t = gpd.GeoSeries(gdf_trios.groupby('labels')['geometry'].last(), crs={'init':'aea'})\n",
    "\n",
    "db_trios_distances = pd.DataFrame(np.vectorize(calc_dist)(dist_1_t, dist_2_t)).rename(columns={0: '1->2'})\n",
    "db_trios_distances['2->3'] = (np.vectorize(calc_dist)(dist_2_t, dist_3_t))\n",
    "db_trios_distances['3->1'] = (np.vectorize(calc_dist)(dist_3_t, dist_1_t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#percentage under 550m for pairs: \n",
    "db_pair_percent_under_550= len(db_distances[db_distances < 550])/len(db_distances)\n",
    "\n",
    "#percentage under 550m for trios: \n",
    "db_dist_for_3 = list(db_trios_distances['1->2'].values) + list(db_trios_distances['2->3'].values) + list(db_trios_distances['3->1'].values)\n",
    "db_dist_for_3 = np.array(db_dist_for_3)\n",
    "db_trio_percent_under_550 = len(db_dist_for_3[db_dist_for_3 < 550])/len(db_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_550, db_trio_percent_under_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Logical Grid \n",
    "\n",
    "two_ids = db[db['cluster_size'] == 2]\n",
    "three_ids =  db[db['cluster_size'] == 3]\n",
    "pair_logical_dist=[]\n",
    "trio_logical_dist_1=[]\n",
    "trio_logical_dist_2=[]\n",
    "trio_logical_dist_3=[]\n",
    "for i in range(len(two_ids)): \n",
    "    id_1 = two_ids['ids'].values[i][0]\n",
    "    id_2 = two_ids['ids'].values[i][1]\n",
    "    pair_logical_dist.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "\n",
    "for i in range(len(three_ids)):\n",
    "    id_1 = three_ids['ids'].values[i][0]\n",
    "    id_2 = three_ids['ids'].values[i][1]\n",
    "    id_3 = three_ids['ids'].values[i][2]\n",
    "    trio_logical_dist_1.append(logical[(logical['level_0'] == id_1) & (logical['level_1'] == id_2)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_2.append(logical[(logical['level_0'] == id_2) & (logical['level_1'] == id_3)]['logical_grid_distance'].values[0])\n",
    "    trio_logical_dist_3.append(logical[(logical['level_0'] == id_3) & (logical['level_1'] == id_1)]['logical_grid_distance'].values[0])\n",
    "\n",
    "    \n",
    "two_ids['logical_distance']= pair_logical_dist\n",
    "three_ids['log_dist_1'] = trio_logical_dist_1\n",
    "three_ids['log_dist_2'] = trio_logical_dist_2\n",
    "three_ids['log_dist_3'] = trio_logical_dist_3\n",
    "db_logical_pairs = two_ids \n",
    "db_logical_trios = three_ids\n",
    "two_ids\n",
    "\n",
    "\n",
    "#calculate the % of outage pairs that are under the same transformer \n",
    "db_pair_percent_under_same_transformer = len(db_logical_pairs[db_logical_pairs['logical_distance'] ==1])/len(db_logical_pairs)\n",
    "\n",
    "#calculate the % of outage trios that are under the same transformer \n",
    "log_dist_for_3 = list(db_logical_trios['log_dist_1']) + list(db_logical_trios['log_dist_2']) + list(db_logical_trios['log_dist_3'])\n",
    "log_dist_for_3 = pd.Series(log_dist_for_3)\n",
    "db_trio_percent_under_same_transformer = len(log_dist_for_3[log_dist_for_3 == 1])/len(log_dist_for_3)\n",
    "\n",
    "db_pair_percent_under_same_transformer, db_trio_percent_under_same_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}