{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime \n",
    "from datetime import timezone\n",
    "#from datetime import datetime\n",
    "#this is all old code that should be deleted eventually \n",
    "#DO NOT RUN \n",
    "pw = pd.read_parquet('part-00000-49a36603-9035-47f4-b73e-eae8d28aa10a-c000.gz.parquet')\n",
    "pw.head()\n",
    "outage = pw[pw['is_powered']]\n",
    "outage = outage[['time', 'location_latitude', 'location_longitude']]\n",
    "outage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outage['time'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jul_1 = outage[outage['time'] < datetime.datetime(2018, 7, 1, 0, 2)]\n",
    "jul_1['time'] = jul_1['time'].apply(lambda x: x.replace(tzinfo=timezone.utc).timestamp())\n",
    "jul_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jul_1['time'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded = pw.join(pd.DataFrame(pw_time['time'].values.tolist(), columns=['year','month', 'day', 'hour', 'min', 'sec', 'sec1', 'sec2', 'sec3'], index=pw_time.index))\n",
    "# expanded = expanded[['powered_longitude', 'powered_latitude', 'outage_longitude', 'outage_latitude', 'year','month', 'day', 'hour', 'min', 'sec']]\n",
    "# expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is running DBSCAN on all outages (not just the transition points)\n",
    "pw_cluster = StandardScaler().fit_transform(jul_1)\n",
    "db = DBSCAN(eps=.65, algorithm='ball_tree').fit(pw_cluster)\n",
    "labels = db.labels_\n",
    "n_noise_ = list(labels).count(-1)\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_, n_clusters_ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "# print(\"Adjusted Rand Index: %0.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
    "# print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#       % metrics.adjusted_mutual_info_score(labels_true, labels,\n",
    "#                                            average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(pw_cluster, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with raw data of outages and times that they went out and then cluster based on outages. then see which ones seem to be the most reliably clustering \n",
    "jul_1['labels'] = labels\n",
    "jul_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jul_1_0 = jul_1[jul_1['labels'] == 0]\n",
    "jul_1_1 = jul_1[jul_1['labels'] == 1]\n",
    "jul_1_2 = jul_1[jul_1['labels'] == 2]\n",
    "jul_1_3 = jul_1[jul_1['labels'] == 3]\n",
    "jul_1_4 = jul_1[jul_1['labels'] == 4]\n",
    "jul_1_5 = jul_1[jul_1['labels'] == 5]\n",
    "jul_1_6 = jul_1[jul_1['labels'] == 6]\n",
    "jul_1_7 = jul_1[jul_1['labels'] == 7]\n",
    "jul_1_8 = jul_1[jul_1['labels'] == 8]\n",
    "jul_1_9 = jul_1[jul_1['labels'] == 9]\n",
    "jul_1_unlabeled = jul_1[jul_1['labels'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to examine a difference in classification, let's zoom in at the data in the top cluster of this plot \n",
    "#print out the distance in time between the points \n",
    "a_time = jul_1[jul_1['location_latitude'] > 5.66]['time'].values[0]\n",
    "for i in jul_1[jul_1['location_latitude'] > 5.66]['time'].values[1:]: \n",
    "    print(abs(a_time - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare that with all the points in cluster 2\n",
    "time1 = jul_1[jul_1['labels'] == 2]['time'].values[0]\n",
    "for i in jul_1[jul_1['labels'] == 2]['time'].values[1:]: \n",
    "    print((time1 - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pw_time['powered_longitude'], pw_time['powered_latitude'], c='b', label='powered')\n",
    "plt.scatter(pw_time['outage_longitude'], pw_time['outage_latitude'], c='r', label='outage')\n",
    "plt.title('Powered Sensors vs Outages')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kneedle analysis to find optimal epsilon \n",
    "ns = 2\n",
    "nbrs = NearestNeighbors(n_neighbors=ns).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "y = np.array(sorted(distances[:,ns-1], reverse=True))\n",
    "x= np.arange(1, len(X)+1)\n",
    "plt.plot(list(range(1,len(X)+1)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more knee \n",
    "from kneed import KneeLocator\n",
    "kneedle = KneeLocator(x, y, S=.001, curve='convex', direction='decreasing')\n",
    "kneedle.plot_knee_normalized()\n",
    "kneedle.plot_knee()\n",
    "print(round(kneedle.knee, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing gradient descent: \n",
    "def average_squared_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the averge squared loss for observations y and predictions y_hat.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return np.mean(np.square(y - y_hat))\n",
    "\n",
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return -2*sum(y*x - theta[0]*x*x - x*np.sin(x*theta[1]))/len(x)\n",
    "    \n",
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    chain_rule = x*np.cos(x*theta[1])\n",
    "    return -2*sum(y*chain_rule - x*chain_rule*theta[0] - chain_rule*np.sin(x*theta[1]))/len(x)\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.array([dt1(x,y,theta), dt2(x,y,theta)])\n",
    "\n",
    "\n",
    "# Then use gradient descent to find the optimal \n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iter): \n",
    "        theta_history.append(theta)\n",
    "        loss_history.append(average_squared_loss(y, [theta[0]]*len(y)))\n",
    "        theta = theta - alpha*(dt(x, y, theta))\n",
    "    \n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t, num_iter=20, alpha=0.1)\n",
    "print(t_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is trash code from STDBSCAN (it's basically trying STDBSCAN on 2 days worth of data to verify that it's clustering well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try it on 2 days of data to do the analysis that I was performing in evaluating_clustering_algorithm\n",
    "days = outages[outages['outage_time'] <= min(outages['outage_time'])+172800]\n",
    "days = pd.DataFrame(test_time(days))\n",
    "days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = days.rename(columns={'cluster': 'labels'})\n",
    "day_a = days[days['labels'] == 0]\n",
    "day_b = days[days['labels'] == 1]\n",
    "day_c = days[days['labels'] == 2]\n",
    "day_d = days[days['labels'] == 3]\n",
    "day_e = days[days['labels'] == 4]\n",
    "day_f = days[days['labels'] == 5]\n",
    "unlabeled = days[days['labels'] == -1]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(unlabeled['location_longitude'], unlabeled['location_latitude'], c='y',label='noise')\n",
    "plt.scatter(day_b['location_longitude'], day_b['location_latitude'], label='cluster 1')\n",
    "plt.scatter(day_c['location_longitude'], day_c['location_latitude'], label='cluster 2')\n",
    "plt.scatter(day_d['location_longitude'], day_d['location_latitude'], label='cluster 3')\n",
    "plt.scatter(day_e['location_longitude'], day_e['location_latitude'], label='cluster 4')\n",
    "#plt.scatter(day_f['location_longitude'], day_f['location_latitude'], label='cluster 5')\n",
    "plt.title('STDBSCAN Clustered Outages from 7/1/18 - 7/2/18')\n",
    "plt.legend()\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "# plt.xlim(left, right)\n",
    "# plt.ylim(top, bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(lst):\n",
    "    return max(lst) - min(lst)\n",
    "\n",
    "#days['time'] = days['time'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "days_clust = pd.DataFrame(days.groupby(['labels'])['time'].apply(lambda x: x.tolist())).rename(columns={'time': 'outage_times'})\n",
    "days_clust['latitude'] = days.groupby(['labels'])['location_latitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust['longitude'] = days.groupby(['labels'])['location_longitude'].apply(lambda x: x.tolist()).values\n",
    "days_clust = days_clust.iloc[1:]\n",
    "\n",
    "days_clust['time_range'] = (np.vectorize(find_range)(days_clust['outage_times']))\n",
    "days_clust['lat_range'] = (np.vectorize(find_range)(days_clust['latitude']))\n",
    "days_clust['long_range'] = (np.vectorize(find_range)(days_clust['longitude']))\n",
    "\n",
    "days_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(days_clust['outage_times'].values[3], label='cluster_4')\n",
    "sns.distplot([1530449581, 1530449581, 1530449582], label='cluster_3')\n",
    "sns.distplot(days_clust['outage_times'].values[1], label='cluster_2')\n",
    "sns.distplot(days_clust['outage_times'].values[0], label='cluster_1')\n",
    "plt.legend()\n",
    "plt.title('STDBSCAN Clustering Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}