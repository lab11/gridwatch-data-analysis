{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "%spark delete -s session1 -q\n",
    "#This cell just gets spark magic working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config \n",
    "{ \"conf\" : {\"spark.driver.extraClassPath\": \"gs://powerwatch-analysis/org.postgresql.jar\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%spark add -s session1 -t None -u http://127.0.0.1:8998 -l python\n",
    "#For this to complete successfully you need to run ./start-cluster.py and wait for it to complete\n",
    "#IF it works it will say \"SparkSession available as spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "#Okay now for the weirdness - When you type \"%%spark\" as above it executes this code in spark on the server cluster\n",
    "#That means this cannot access any variables that are in the local context (any cell without %%spark)\n",
    "#You CAN get data frames back from a %%spark cell by putting \"%%spark -o <dataframe_name>\"\n",
    "\n",
    "#This is notably problematic!\n",
    "#First - how do we transfer config to the remote server?\n",
    "#For now ./start-cluster.py will transfer a config.json file to the servers storage bucket\n",
    "#you can get it like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o pw_df\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, asc, desc, lead, lag, udf, hour, month, dayofmonth, dayofyear, collect_list, lit, year, date_trunc, dayofweek, when, unix_timestamp, array\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType, DateType, TimestampType, LongType\n",
    "from pyspark import SparkConf\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from math import isnan\n",
    "import argparse\n",
    "import json\n",
    "import calendar\n",
    "\n",
    "import os\n",
    "import cloudstorage as gcs\n",
    "import webapp2\n",
    "\n",
    "from google.appengine.api import app_identity\n",
    "\n",
    "### It's really important that you partition on this data load!!! otherwise your executors will timeout and the whole thing will fail\n",
    "start_time = '2018-07-01'\n",
    "end_time = '2019-09-01'\n",
    "cluster_distance_seconds = 180\n",
    "CD = cluster_distance_seconds\n",
    "\n",
    "#Roughly one partition per week of data is pretty fast and doesn't take too much chuffling\n",
    "num_partitions = int((datetime.strptime(end_time,\"%Y-%m-%d\").timestamp() - datetime.strptime(start_time,\"%Y-%m-%d\").timestamp())/(7*24*3600))\n",
    "\n",
    "# This builds a list of predicates to query the data in parrallel. Makes everything much faster\n",
    "start_time_timestamp = calendar.timegm(datetime.strptime(start_time, \"%Y-%m-%d\").timetuple())\n",
    "end_time_timestamp = calendar.timegm(datetime.strptime(end_time, \"%Y-%m-%d\").timetuple())\n",
    "stride = (end_time_timestamp - start_time_timestamp)/num_partitions\n",
    "predicates = []\n",
    "for i in range(0,num_partitions):\n",
    "    begin_timestamp = start_time_timestamp + i*stride\n",
    "    end_timestamp = start_time_timestamp + (i+1)*stride\n",
    "    pred_string = \"time >= '\" + datetime.utcfromtimestamp(int(begin_timestamp)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    pred_string += \"' AND \"\n",
    "    pred_string += \"time < '\" + datetime.utcfromtimestamp(int(end_timestamp)).strftime(\"%Y-%m-%d %H:%M:%S\") + \"'\"\n",
    "    predicates.append(pred_string)\n",
    "\n",
    "#This query should only get data from deployed devices in the deployment table\n",
    "query = (\"\"\"\n",
    "    (SELECT powerwatch.core_id, time, is_powered, product_id, millis, last_unplug_millis,\n",
    "            last_plug_millis, d.location_latitude, d.location_longitude, d.site_id FROM\n",
    "    powerwatch\n",
    "    INNER JOIN (\n",
    "      SELECT core_id,\n",
    "        location_latitude,\n",
    "        location_longitude,\n",
    "        CAST(site_id as INTEGER) as site_id,\n",
    "        COALESCE(deployment_start_time, '1970-01-01 00:00:00+0') as st,\n",
    "        COALESCE(deployment_end_time, '9999-01-01 00:00:00+0') as et\n",
    "      FROM deployment) d ON powerwatch.core_id = d.core_id\n",
    "    WHERE time >= st AND time <= et AND site_id < 100 AND \"\"\" +\n",
    "        \"time >= '\" + start_time + \"' AND \" +\n",
    "        \"time < '\" + end_time + \"' AND \" +\n",
    "        \"(product_id = 7008 OR product_id = 7009 or product_id = 7010 or product_id = 7011 or product_id = 8462)) alias\")\n",
    "\n",
    "pw_df = spark.read.jdbc(\n",
    "            url = \"jdbc:postgresql://timescale.ghana.powerwatch.io/powerwatch\",\n",
    "            table = query,\n",
    "            predicates = predicates,\n",
    "            properties={\"user\": args.user, \"password\": args.password, \"driver\":\"org.postgresql.Driver\"})\n",
    "\n",
    "#if you have multiple saves below this prevents reloading the data every time\n",
    "pw_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o test\n",
    "import sys\n",
    "sys.version\n",
    "test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}